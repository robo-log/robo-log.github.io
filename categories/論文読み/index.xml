<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>論文読み on ROBO LOG</title>
    <link>http://blog.syundo.org/categories/%E8%AB%96%E6%96%87%E8%AA%AD%E3%81%BF/</link>
    <description>Recent content in 論文読み on ROBO LOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 10 Nov 2018 18:15:40 +0900</lastBuildDate>
    
	<atom:link href="http://blog.syundo.org/categories/%E8%AB%96%E6%96%87%E8%AA%AD%E3%81%BF/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>論文 Exploration by Random Network Distillation</title>
      <link>http://blog.syundo.org/post/20181107-exploration-by-random-network-distillation/</link>
      <pubDate>Sat, 10 Nov 2018 18:15:40 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20181107-exploration-by-random-network-distillation/</guid>
      <description> Montezuma’s RevengeというDeep RLで従来クリアが難しいとされていたゲームのスコアを大幅に更新したOpenAIの研究。
 OpenAIのブログはこちら https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/ 論文はこちら https://arxiv.org/abs/1810.12894 実装も公開されている https://github.com/openai/random-network-distillation  どのくらいスコアを更新したかというとこのくらい。
本研究で学習するnetworkとしては内発的な報酬を決めるためのnetworkと方策を決めるためのnetworkがある。 この2つの組み合わせで探索したことの無い状態を見つけ出すように行動し、同時に高い報酬も得るようになる。
内発的な報酬を決めるnetworkの学習 観測から潜在変数を推定する、ある関数$f(x)$があるとする。 これはNeural Networkで構築されており、これをtarget networkと呼ぶ。 target networkが正しく潜在表現を抽出できていると仮定するならば、推定関数$\hat{f}(x;\theta)$は以下の最小2乗誤差で評価できる。
\begin{equation} |\hat{f}(x;\theta) - f(x)|^2 \end{equation}
仮定したtarget networkであるが、そんなものが分かるわけがないので、本研究ではパラメタをランダムに初期化したneural networkをtarget networkとしている。 この誤差を最小化する過程が、論文タイトルにある、Random Network　Distillation(ランダムなネットワークに対する蒸留)なのである。 しかし本当に潜在変数を求めたいのであれば、Random Network　Distillationには何の意味も無いように思える。 教師あり学習でなく、VAEなどの教師なし学習の手法が使われるだろう。 ではなぜ、Distillationをするかというと、その誤差が&amp;rdquo;探索ボーナス&amp;rdquo;として使えるからである。 経験したことがない観測かどうかを評価したいときに、従来、状態への到達数のカウントなどが用いられたが、Distillationの誤差を使えばそれと同じような効果が得られるのである。
潜在変数の推定networkの更新は方策networkの更新と同時に行われる。 それによって、行動による経験の度合いとして対応付けることができる。
方策networkの学習 方策networkの学習にはPPOが用いられた。
報酬としては内発的な報酬 \begin{equation} i_t = |\hat{f}(s_{t+1}) - f(s_{t+1})|^2 \end{equation} とゲームから得られる通常の外発的な報酬$e_t$が用いられる。
この報酬を一定のホライズンについて記録して累積報酬$R_{I, i}$、$R_{E, i}$、そしてアドバンテージ$A_{I, i}$、$A_{E, i}$を求める。 これがPPOの過程で用いられる。
ただし、潜在変数推定networkはゲームの盤面が変わっても同じものが用いられる。 そのときに、networkの出力はゲームの盤面によってスケールが変わってしまうので、normalizationが必要になる。 target networkと推定networkに入力される観測について平均値のバイアスを引くのと一定の値域に収まるようにscalingすることによって潜在変数networkのnormalizationとしている。
動画  </description>
    </item>
    
    <item>
      <title>論文 Asynchronous Methods for Deep Reinforcement Learning</title>
      <link>http://blog.syundo.org/post/20180303-asynchronous-methods-for-deep-reinforcement-learning/</link>
      <pubDate>Sat, 03 Mar 2018 16:28:53 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20180303-asynchronous-methods-for-deep-reinforcement-learning/</guid>
      <description>Asynchronous Methods for Deep Reinforcement Learning を読んだ。
Arxivリンク https://arxiv.org/abs/1602.01783
Actor-Criticを並列に学習することによって、Atariのタスクにおいて好成績を出せるようになったというのがこの論文のcontribute。 いわゆるA3Cと呼ばれるアルゴリズムにあたる。
方策オン型の強化学習は、更新前後の方策が時間的に強く相関しているために、安定して収束しないというのは共通認識であった。 そこで、方策オフ型の手法とExperience Bufferを使って、相関を除去するという手法が取られていた。 しかし、このような手法は、方策オフ型の学習に限られてしまうし、経験の保持のために多くのメモリを必要としてしまう。
そこで、復数のエージェントを並列で走らせ、その結果を使って学習することで時間的相関を緩和する手法が考えられた。 これはSarsaやActor-Criticのような方策オン型学習にも、Q-Learningのような方策オフ型の手法にも、もちろんDeep Learningを使う方法にも適用できる考え方である。 この方法の副次的な利点として、従来は１台の強力なGPUを持ったコンピュータを使ってExperience Replayを行う必要があったが、分散的に学習できるようになったために、通常のCPUを持った復数のコンピューターがあれば性能の高い学習を実現することができる、ということも確認されている。
学習手法 コンポーネントとしては、複数のActor-Learnerがあり、そのパラメタを動悸する Actor-Learner では数ステップ分の勾配を蓄積する。 これによって、Actor-Learnerがお互いにパラメタを上書きしてしまうのを防止する。 蓄積をどれだけするかというのは、計算量とデータ量のどちらを優先するかというトレードオフになる。 蓄積してバッチ的にパラメタを更新することで計算量を減らすことができるが、一方で学習のために必要なデータ量が増えるということである。
探索 各Actor-Learnerでの探索の方法は、それぞれ違っていたほうがロバスト性の高い方策を学習できるという。 この論文では、$\epsilon$-greedy 法において各Learnerで$\epsilon$をある分布からサンプルして決定することでLearnerごとの探索方法に差異をもたせた。
また、目的関数に方策$\pi$のエントロピーが用いられている。 目的関数にエントロピーの項を加え、正則化項として働かせることで、すぐに局所最適解に陥ってしまうのを防ぐ効果があることが、(Williams &amp;amp; Peng, 1991)によって確認されている。
実験  57 Atari games TORCS Car Racing Simulator Continuous Action Control Using the MuJoCo Physics Simulator Labyrinth  の課題を解いた。 16個のCPUを用いて、当時のSOTAを達成した。</description>
    </item>
    
    <item>
      <title>論文 Learning Complex Dexterous Manipulation With Deep Reinforcement Learning and Demonstrations</title>
      <link>http://blog.syundo.org/post/20180120-learning-complex-dexterous-manipulation-with-deep-reinforcement-learning-and-demonstrations/</link>
      <pubDate>Sat, 20 Jan 2018 20:15:22 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20180120-learning-complex-dexterous-manipulation-with-deep-reinforcement-learning-and-demonstrations/</guid>
      <description>Learning Complex Dexterous Manipulation With Deep Reinforcement Learning and Demonstrations
arxivを読む。
以下の動画のようにハンドを使った複雑なタスクの学習を少ない学習ステップ数で実現している。 人間のデモンストレーションを学習に用いることで、他手法と比べて人間らしい動作になっている。 例えば変に指先が曲がったりしていない。
 この論文では、
 自然方策勾配法を使って、方策の学習をしている 方策をbehavior closingを使って事前学習している RLのプロセスにおける方策の更新でも人間によるデモとの類似を使っている  behavior closingはimitation learningの一種の方法で、人間によるデモを再現できるように方策を教師あり学習することである。
事前学習が終わったらMDPにおける学習を始める。 方策勾配は以下のようにする。
\begin{equation} g_{aug} = \sum_{(s, a) \in \rho_{\pi}} \nabla_{\theta} \log \pi_{\theta} (a | s) A^{\pi}(s, a) + \sum_{(s, a) \in \rho_D} \nabla_{\theta} \log \pi_{\theta} (a | s) w(s, a) \end{equation}
ここで、$rho_{\pi}$はMDPにおいて得られたデータセット、$\rho_D$は事前学習に用いたデータセットである。 $w(s, a)$は重みづけ関数であり、スケール的には $w(s, a) = A^{\pi}(s,a) \hspace{5mm} \forall (s, a) \in \rho_D$を用いるのが妥当だが、計算できないので、ヒューリスティック的に以下を用いる。
\begin{equation} w(s, a) = \lambda_0 \lambda_1^k \max_{(s&amp;rsquo;, a&amp;rsquo;) \in \rho_{\pi}} A^{\pi}(s&amp;rsquo;, a&amp;rsquo;) \hspace{5mm} \forall (s, a) \in \rho_D \end{equation}</description>
    </item>
    
  </channel>
</rss>