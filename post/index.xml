<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ROBO LOG</title>
    <link>http://blog.syundo.org/post/</link>
    <description>Recent content in Posts on ROBO LOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 17 Nov 2017 03:15:08 +0900</lastBuildDate>
    <atom:link href="http://blog.syundo.org/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>強化学習についてまとめる (3) 方策勾配に基づくアルゴリズム</title>
      <link>http://blog.syundo.org/post/20171117-Reinforcement-Learning-Policy-Gradient/</link>
      <pubDate>Fri, 17 Nov 2017 03:15:08 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20171117-Reinforcement-Learning-Policy-Gradient/</guid>
      <description>

&lt;!-- 強化学習について古典的なものからDeepNNを使ったものまでまとめていきたい。
&lt;!-- ![](/images/2016/04/lis.gif) --&gt;

&lt;!-- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)を土台にして他文献を参照しながら構成していく。
取り上げる順番はこだわらず、自分のわかりやすい方法を採る。
# ロードマップ  --&gt;

&lt;p&gt;&lt;a href=&#34;../20171110-Reinforcement-Value-Policy-Iteration&#34;&gt;前回、&lt;/a&gt;
&lt;a href=&#34;../20160410-Reinforcement-Learning-MDP-Belman-Equation&#34;&gt;前々回&lt;/a&gt;
では、価値関数を求め、それを基に行動を決定する手法について扱ってきた。
しかし、そもそもロボットの行動を決めるのは方策であるのだから、その方策を直接学習できないのだろうか？という疑問が湧く。&lt;/p&gt;

&lt;p&gt;今回は、前回までとは全く違うアプローチとして、&lt;strong&gt;方策勾配法&lt;/strong&gt; をまとめる。
方策勾配法は、方策をあるパラメタで表される関数とし、そのパラメタを学習することで、直接方策を学習していくアプローチである。&lt;/p&gt;

&lt;p&gt;方策を直接扱うことで&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$V^{\pi}$や$Q^{\pi}$を求めるような複雑でメモリを消費する手法を使わなくて良い&lt;/li&gt;
&lt;li&gt;連続空間での行動が扱いやすくなる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などの利点がある。
一方で、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;方策オン型の学習になるため、行動のタイミングで方策を更新しなくてはならず、学習効率が落ちる&lt;/li&gt;
&lt;li&gt;行動に対して学習が敏感に影響を受けてしまうため、学習ステップの決定が難しい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などの欠点がある。&lt;/p&gt;

&lt;h1 id=&#34;方策のモデルと勾配:d282ab77251597e53cc889d5b2950abb&#34;&gt;方策のモデルと勾配&lt;/h1&gt;

&lt;p&gt;$\theta$でパラメタライズされた確率的な方策$\pi_{\theta}$を求める問題を考える。
$\tau$をステップ$0$から$H$までの状態-行動の系列(状態-行動空間でのパス)$\tau=(s_0, a_0, \dots, s_H, a_H)$としたとき、方策の評価関数として以下を考える。&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
U(\theta) &amp;amp;=&amp;amp; E [\sum_{t=0}^H R(s_t, u_t) ; \pi_{\theta}] \\
&amp;amp;=&amp;amp; \sum_{\tau} P(\tau ; \theta) R(\tau)
\end{aligned}
\end{equation}
ここで、$R(\tau) = \sum_{t=0}^H R(s_t, u_t)$としている。
また$P(\tau ; \theta)$はパスの生成モデルであり、定義より
\begin{equation}
P(\tau ; \theta) = \prod_{t=0}^H P(s_{t+1} | s_t, u_t) \pi_{\theta}(u_t | s_t)
\label{eq:p_tau_theta}
\end{equation}
である。&lt;/p&gt;

&lt;p&gt;以上の設定において、方策の学習は最終的に
\begin{equation}
\max_{\theta} U(\theta) = \max_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau)
\end{equation}
を求める問題となる。
そこで、微小ステップごとに評価関数の$\theta$での微分方向
\begin{equation}
\nabla_{\theta} U(\theta) = \nabla_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau)
\end{equation}
に方策を更新することで、方策を最適化することを考える。
これは以下のように変形できる。&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
\nabla_{\theta} U(\theta) &amp;amp;=&amp;amp; \nabla_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau) \\
&amp;amp;=&amp;amp; \sum_{\tau} \nabla_{\theta} P(\tau ; \theta) R(\tau) \\
&amp;amp;=&amp;amp; \sum_{\tau} \frac{P(\tau ; \theta)}{P(\tau ; \theta)} \nabla_{\theta} P(\tau ; \theta) R(\tau) \\
&amp;amp;=&amp;amp; \sum_{\tau} P(\tau ; \theta) \frac{\nabla_{\theta} P(\tau ; \theta)}{P(\tau ; \theta)} R(\tau) \\
&amp;amp;=&amp;amp; \sum_{\tau} P(\tau ; \theta) \nabla_{\theta} \log P(\tau ; \theta) R(\tau) \\
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;$P(\tau ; \theta)$で期待値を取る操作は、以下の平均を取る操作として近似する。
\begin{equation}
\nabla_{\theta} U(\theta)
\approx
\hat{g} = \frac{1}{m} \sum_{i=0}^m \nabla_{\theta} \log P(\tau^{(i)} ; \theta) R(\tau^{(i)})
\end{equation}&lt;/p&gt;

&lt;p&gt;この式を見ると、更新則によって、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;報酬$R$が高いパスの存在確率を上げる&lt;/li&gt;
&lt;li&gt;報酬$R$が低いパスの存在確率を下げる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;方向に方策が更新されることがわかる。
これをパスではなく方策と遷移確率について立式するとどうなるだろうか。
式\eqref{eq:p_tau_theta}より、&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
\nabla_{\theta} \log P(\tau^{(i)} ; \theta)
&amp;amp;=&amp;amp; \nabla_{\theta} \log
\left[ \prod_{t=0}^H P(s_{t+1}^{(i)} | s_t^{(i)}, u_t^{(i)}) \pi_{\theta}(u_t^{(i)} | s_t^{(i)}) \right] \\
&amp;amp;=&amp;amp; \nabla_{\theta} \left[ \sum_{t=0}^H \log P(s_{t+1}^{(i)} | s_t^{(i)}, u_t^{(i)}) \right] +
\nabla_{\theta} \left[ \sum_{t=0}^H \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) \right] \\
&amp;amp;=&amp;amp; \nabla_{\theta} \sum_{t=0}^H \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) \\
&amp;amp;=&amp;amp; \sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)})
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;となる。状態遷移確率$P(s_{t+1}^{(i)} | s_t^{(i)}, u_t^{(i)})$は$\theta$をパラメタとして持たないので、$\theta$で微分するとこの項が消えるのである。&lt;/p&gt;

&lt;p&gt;結局、勾配は
\begin{equation}
\begin{aligned}
\hat{g} = \frac{1}{m} \sum_{i=0}^m \nabla_{\theta} \log P(\tau^{(i)} ; \theta) R(\tau^{(i)}) \\
where \hspace{15pt}
\nabla_{\theta} \log P(\tau^{(i)} ; \theta) =
\sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)})
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;すなわち
\begin{equation}
\hat{g} = \frac{1}{m} \sum_{i=0}^m \sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) R(\tau^{(i)})
\label{eq:policy_gradient}
\end{equation}&lt;/p&gt;

&lt;p&gt;で表され、方策の$\theta$での勾配のみで表現できることがわかる。&lt;/p&gt;

&lt;h2 id=&#34;baseline:d282ab77251597e53cc889d5b2950abb&#34;&gt;Baseline&lt;/h2&gt;

&lt;p&gt;式\eqref{eq:policy_gradient}にbaseline $b$という値を追加する。
\begin{equation}
\hat{g} = \frac{1}{m} \sum_{i=0}^m \sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) (R(\tau^{(i)}) - b)
\label{eq:base_lined_policy_gradient}
\end{equation}&lt;/p&gt;

&lt;p&gt;baselineは$\hat{g}$の値には影響を与えないが、調整前に比べて$\nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) (R(\tau^{(i)}) - b)$の分散を減少させる効果がある。&lt;/p&gt;

&lt;p&gt;$R(\tau^{(i)}) - b$が小さいほど分散が小さくなるので、$b$の決定法としては、$R(\tau^{(i)})$との二乗距離を小さくするように調整すればよい。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>強化学習についてまとめる (2) 反復による価値の推定</title>
      <link>http://blog.syundo.org/post/20171110-Reinforcement-Value-Policy-Iteration/</link>
      <pubDate>Fri, 10 Nov 2017 19:54:59 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20171110-Reinforcement-Value-Policy-Iteration/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;../20160410-Reinforcement-Learning-MDP-Belman-Equation&#34;&gt;前回&lt;/a&gt;はMDPとベルマン方程式について扱った。
ベルマン方程式を解くことができれば、$Q^{\pi}$を計算できるのだが、どう計算するのか、価値関数からどのように方策を決定するのかという問題がある。
今回は、多数のデータを使って反復的に計算することでこれを求める方法について扱いたい。
&lt;!-- 上式を見ればわかるように、そのためには状態遷移確率が既知である必要がある。
扱いたいのは環境が未知の問題であるため、状態遷移確率を用いずに反復的に価値関数を求める。 --&gt;&lt;/p&gt;

&lt;h1 id=&#34;価値推定と反復:0609effec2e538d5a738f7edd8890087&#34;&gt;価値推定と反復&lt;/h1&gt;

&lt;p&gt;価値関数についてのベルマン方程式において、常に最適な方策を取るという前提を置けば、以下の&lt;strong&gt;最適ベルマン方程式&lt;/strong&gt;を定めることができる。
\begin{equation}
\begin{aligned}
V^{\pi}(s) = \max_{a \in A} \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma V^{\pi}(s&amp;rsquo;) \right)
\end{aligned}
\label{eq:belman_value_func_max}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
Q^{\pi}(s, a) = \sum_{s&amp;rsquo;} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma \max_{a&amp;rsquo; \in A} Q^{*}(s&amp;rsquo;,a&amp;rsquo;) \right)
\end{aligned}
\label{eq:belman_q_func_max}
\end{equation}&lt;/p&gt;

&lt;p&gt;あるいは、取り得る方策が確率的でない、常に方策が決まっている定常方策を取るとすると、以下のようになる。
\begin{equation}
\begin{aligned}
V^{\pi}(s) =  \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma V^{\pi}(s&amp;rsquo;) \right)
\end{aligned}
\label{eq:belman_value_func_const}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
Q^{\pi}(s, a) = \sum_{s&amp;rsquo;} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma Q^{\pi}(s&amp;rsquo;,a&amp;rsquo;) \right)
\end{aligned}
\label{eq:belman_q_func_const}
\end{equation}&lt;/p&gt;

&lt;p&gt;以上の性質は価値関数を逐次的に更新していくことで求めていくために重要になる。
また、その価値関数に従って最適な方策を決定し行動していくことで逐次的に方策を更新していくこともできる。
前者を価値反復、後者を方策反復と呼ぶ。
このとき、方策の決定方法としては以下のようなgreedy方策が考えられる。
$$
a^* = \arg\max(Q^{*})
$$&lt;/p&gt;

&lt;p&gt;また、実際に採用している方策と異なる方策を学習する方法は&lt;strong&gt;方策オフ型&lt;/strong&gt;学習と呼ばれる。
一方で、学習した方策をそのときどき採用する方法を&lt;strong&gt;方策オン&lt;/strong&gt;型学習と呼ぶ。&lt;/p&gt;

&lt;h2 id=&#34;td学習:0609effec2e538d5a738f7edd8890087&#34;&gt;TD学習&lt;/h2&gt;

&lt;p&gt;MDPにおいて価値関数を推定したい。
TD学習(Temporal difference learning)は、現在の推定値を学習中の目標値として使用することで、問題を解いていく手法である。&lt;/p&gt;

&lt;p&gt;ベルマン方程式は、状態遷移確率が未知の場合、そのまま解くことはできない。
そこで、状態遷移確率は実際に観測する(サンプリングする)ことによって、確率分布を近似し、ベルマン方程式を扱う。&lt;/p&gt;

&lt;h3 id=&#34;td-0-法:0609effec2e538d5a738f7edd8890087&#34;&gt;TD(0)法&lt;/h3&gt;

&lt;p&gt;以下のように価値関数の推定値を更新していく手法がTD(0)法である。
\begin{equation}
V_{t+1}(s_t)
\leftarrow
(1 - \alpha_t) V_t(s_t) + \alpha_t (R_{t+1} + \gamma V_t(s_{t+1}))
\end{equation}&lt;/p&gt;

&lt;p&gt;現在の値$V_t(s_t)$と目標値$R_{t+1} + \gamma V_t(s_{t+1})$との内分によって推定値を更新していくのがこの手法である。
&lt;strong&gt;TD&lt;/strong&gt; 誤差として$\delta_t$を以下のように定義することで、TD誤差を小さくする方向に更新するアルゴリズムとして捉えることもできる。&lt;/p&gt;

&lt;p&gt;\begin{equation}
\delta_t = (R_{t+1} + \gamma V_t(s_{t+1})) - V_t(s_t)\\
V_{t+1}(s_t)
\leftarrow
V_t(s_t) + \alpha_t \delta_t
\end{equation}&lt;/p&gt;

&lt;p&gt;$\delta_t$は目標値と現在の値の差分であるから、この立式のほうがわかりやすいかもしれない。&lt;/p&gt;

&lt;h3 id=&#34;sarsa:0609effec2e538d5a738f7edd8890087&#34;&gt;Sarsa&lt;/h3&gt;

&lt;p&gt;TD(0)法を行動価値数に拡張したものが、Sarsaである。&lt;/p&gt;

&lt;p&gt;時刻$t$において状態$s_t$であり$a_t$を行った結果、次の状態$s_{t+1}$と報酬$r_t$を観測したとき、$s_{t+1}$において行う予定の行動$a_{t+1}$をもとに、以下の更新則でQ値を更新する。&lt;/p&gt;

&lt;p&gt;\begin{equation}
Q(s_t, a_t)
\leftarrow
(1 - \alpha) Q(s_t, a_t) + \alpha (r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}))
\end{equation}&lt;/p&gt;

&lt;h3 id=&#34;q-learning:0609effec2e538d5a738f7edd8890087&#34;&gt;Q-Learning&lt;/h3&gt;

&lt;p&gt;最適ベルマン方程式を解くためにで、以下の更新則でQ値を更新するのがQ-Learningである。&lt;/p&gt;

&lt;p&gt;\begin{equation}
Q(s_t, a_t)
\leftarrow
(1 - \alpha) Q(s_t, a_t) + \alpha (r_{t+1} + \gamma \max_{a^{\prime} \in A} Q(s_{t+1}, a^{\prime}))
\end{equation}&lt;/p&gt;

&lt;p&gt;Sarsaと違って、次に何の行動を取ったかはQ値の更新には関わってこない。
$\max_{a^{\prime} \in A} Q(s_{t+1}, a^{\prime})$つまり、次の状態において取れる行動のうち、最大の価値を得られる行動(つまり最適方策$\pi^{*}$)を採った時のQ値を使って更新する。
Sarsaが実際に取った方策に更新が依存するのに対して、Q-LearningではQ値は環境に対して一定の値に収束する、方策オフ型学習である。&lt;/p&gt;

&lt;!-- ### 方策の決定方法
Q値が求まったら、最も高いQ値を持つ行動$a$を選択すれば、将来にわたって最大の報酬が得られることが期待できる。
つまり、行動$a^{\*}$は


となる方策を採用するということになる。
これはgreedy方策と呼ばれている。

さて、この方策では$Q^{\*}$がわかっていれば最適な方策となるが、強化学習では行動をしながら$Q$を更新していくので$Q^{\*}$は常に不明である。
探索をするために、ある程度ランダムな方策を取る必要がある。
そのように確率$\epsilon$でランダムな行動を取る方策を$\epsilon$-greedy方策という。 --&gt;
</description>
    </item>
    
    <item>
      <title>強化学習についてまとめる (1) MDPとベルマン方程式</title>
      <link>http://blog.syundo.org/post/20160410-Reinforcement-Learning-MDP-Belman-Equation/</link>
      <pubDate>Sun, 10 Apr 2016 15:19:17 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20160410-Reinforcement-Learning-MDP-Belman-Equation/</guid>
      <description>

&lt;p&gt;強化学習について古典的なものからDeepNNを使ったものまでまとめていきたい。&lt;/p&gt;

&lt;h1 id=&#34;マルコフ決定過程:71088a822ab0e727ae2c68c7ef7c6e1e&#34;&gt;マルコフ決定過程&lt;/h1&gt;

&lt;p&gt;マルコフ決定過程(Markov Decision Process; MDP)は状態の遷移が確率的に起こり、マルコフ過程を満たす過程のことをいう。
MDPは状態$s$、行動$a$、遷移先の状態を$s&amp;rsquo;$、状態遷移確率$P(s&amp;rsquo;|s, a)$の組で表現される。
また、状態$s$において行動$a$を選択したとき、即時報酬$r(s, a, s&amp;rsquo;)$が得られるとする。&lt;/p&gt;

&lt;p&gt;とくに時間的な過程の進展を表すため、特に時刻$t$から$t+1$の状態の遷移について&lt;/p&gt;

&lt;p&gt;$$
状態: s_t\
行動: a_t\
状態遷移確率: P(s_{t+1}|s_t, a_t)\
報酬関数: r_t = r(s_t, a_t, s_{t+1})
$$&lt;/p&gt;

&lt;p&gt;を考える。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.syundo.org/images/2017/11/MDP.png&#34; alt=&#34;マルコフ決定過程&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;価値関数:71088a822ab0e727ae2c68c7ef7c6e1e&#34;&gt;価値関数&lt;/h1&gt;

&lt;p&gt;時刻$t$において将来($t \rightarrow \infty $)にわたって得られる報酬について、割引累積報酬$G_t$を定義する。
\begin{equation}
G_t = \sum_{k=0}^{\infty} \gamma ^k R_{t+k+1}
\end{equation}
ここで、$R_{t+1}$は$r(s_t, a_t, s_{t+1})$の値とする。
また、$\gamma$は$0 \le \gamma &amp;lt; 1$の値で、遠い将来に得られるであろう報酬を低く見積もるために使う。&lt;/p&gt;

&lt;p&gt;状態$s$において、行動$a$が選択される確率を$\pi = \pi(a | s)$とする。
この$\pi$を&lt;strong&gt;方策&lt;/strong&gt;と呼ぶ。
ロボットで言うと次の状態をどう選ぶかの判断を行う部分である。&lt;/p&gt;

&lt;p&gt;さて、ある方策$\pi$を採用したときの報酬がどの程度のものか見積もりたい。
方策$\pi$のもとで、以下のように関数$V^{\pi}$を定義する。
\begin{equation}
V^{\pi}(s) = E[G_t|s_t=s] = E[R_{t+1}\ + \gamma R_{t+2} + \dots | s_t = s]
\label{eq:v_func}
\end{equation}
これを&lt;strong&gt;状態価値関数&lt;/strong&gt;(あるいは単に&lt;strong&gt;価値関数&lt;/strong&gt;)と呼ぶ。
期待値を取っているのは、方策$\pi$は確率的であるから、$s_t=s$となるのも確率的であるため、$s_t$について周辺化して評価したいためである。&lt;/p&gt;

&lt;p&gt;同様に、状態だけでなく、行動についても条件として
\begin{equation}
Q^{\pi}(s,a) = E[G_t|s_t=s, a_t=a] = E[R_{t+1}\ + \gamma R_{t+2} + \dots | s_t = s, a_t = a]
\label{eq:q_func}
\end{equation}
も考える。これを、&lt;strong&gt;行動価値関数&lt;/strong&gt; と呼ぶ。&lt;/p&gt;

&lt;p&gt;以上の枠組みにおいて、最も良い方策、&lt;strong&gt;最適方策&lt;/strong&gt; を$\pi^{*}$とし、
この方策を採用したときの価値関数を&lt;strong&gt;最適行動価値関数&lt;/strong&gt;
\begin{equation}
Q^{*}(s,a) = Q^{\pi^{*}}(s,a) = \max_{\pi} Q^{\pi}(s,a)
\end{equation}
とする。&lt;/p&gt;

&lt;h2 id=&#34;ベルマン方程式の導出:71088a822ab0e727ae2c68c7ef7c6e1e&#34;&gt;ベルマン方程式の導出&lt;/h2&gt;

&lt;p&gt;\eqref{eq:v_func}式において$E[*]$は線形の演算のため、&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
V^{\pi}(s) &amp;amp;=&amp;amp;  E[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | s_t = s]\\
&amp;amp;=&amp;amp; E[R_{t+1}\ | s_t = s] + E[\sum_{k=1}^{\infty} \gamma^k R_{t+k+1} | s_t = s]\\
&amp;amp;=&amp;amp; E[R_{t+1}\ | s_t = s] + \gamma E[\sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1} | s_t = s]
\end{aligned}
\label{eq:q_func_trans}
\end{equation}&lt;/p&gt;

&lt;p&gt;とすることができる。&lt;/p&gt;

&lt;p&gt;ここで、\eqref{eq:q_func_trans}式右辺第1項は&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
E[R_{t+1}\ | s_t = s]
=\sum_{a \in A(s)} \pi(a|s) \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s, a) r(s, a, s&amp;rsquo;)
\end{aligned}
\label{eq:q_func_trans_1}
\end{equation}&lt;/p&gt;

&lt;p&gt;となる。
$s_t=s$において行動$a$を取る確率が$\pi(a,|s)$、状態遷移して$s&amp;rsquo;$に移動する確率が$P(s&amp;rsquo;|s,a)$であるから、行動$a$を取って、状態$s&amp;rsquo;$に遷移する確率は$\pi(a,|s)P(s&amp;rsquo;|s,a)$である。
その上で、状態$s$において取れる行動の全集合$A(s)$と次に取れる全状態$S$について$r(s,a,s&amp;rsquo;)$の期待値を取る、というのが上式で行われていることである。&lt;/p&gt;

&lt;p&gt;次に、\eqref{eq:q_func_trans}式右辺第2項は
\begin{equation}
\begin{aligned}
E[\sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1} | s_t = s]
&amp;amp;=&amp;amp; \sum_{a \in A(s)} \pi(a|s) \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s,a) E[\sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1} | s_{t+1} = s&amp;rsquo;]\\
&amp;amp;=&amp;amp; \sum_{a \in A(s)} \pi(a|s) \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s,a) E[\sum_{k=0}^{\infty} \gamma^k R_{(t+1)+k+1} | s_{t+1} = s&amp;rsquo;]\\
&amp;amp;=&amp;amp; \sum_{a \in A(s)} \pi(a|s) \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s,a) V^{\pi}(s&amp;rsquo;)
\end{aligned}
\label{eq:q_func_trans_2}
\end{equation}&lt;/p&gt;

&lt;p&gt;と変形できる。
以上\eqref{eq:q_func_trans}, \eqref{eq:q_func_trans_1}, \eqref{eq:q_func_trans_2} 式により、&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned}
V^{\pi}(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma V^{\pi}(s&amp;rsquo;) \right)
\end{aligned}
\label{eq:belman_value_func}
\end{equation}&lt;/p&gt;

&lt;p&gt;が導出される。これを&lt;strong&gt;ベルマン方程式&lt;/strong&gt;と呼ぶ。&lt;/p&gt;

&lt;p&gt;また、$Q^{\pi}(s,a)$の定義より
\begin{equation}
\begin{aligned}
V^{\pi}(s) = \sum_{a \in A(s)} \pi(a|s) Q^{\pi}(s,a)
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;であるから、
\begin{equation}
\begin{aligned}
Q^{\pi}(s, a) &amp;amp;=&amp;amp; \sum_{s&amp;rsquo;} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma V^{\pi}(s&amp;rsquo;) \right)\\
&amp;amp;=&amp;amp; \sum_{s&amp;rsquo;} P(s&amp;rsquo;|s, a) \left(r(s, a, s&amp;rsquo;) + \gamma \sum_{a \in A(s&amp;rsquo;)} \pi(a&amp;rsquo;|s&amp;rsquo;) Q^{\pi}(s&amp;rsquo;,a&amp;rsquo;) \right)
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;と行動価値関数についてのベルマン方程式を導出できる。&lt;/p&gt;

&lt;!-- 上記のベルマン方程式において、常に最適な方策を取るという前提を置けば、以下のベルマン最適方程式が

さて、このベルマン方程式を解けば、$Q^{\pi}$を計算できるのだが、上式を見ればわかるように、そのためには状態遷移確率が既知である必要がある。
扱いたいのは環境が未知の問題であるため、状態遷移確率を用いずに反復的に価値関数を求める。

## 価値反復を使ったアルゴリズム
状態遷移確率が未知であるため、実際に行動してみて、状態遷移確率分布からサンプリングすることで価値観数を求めていくのが、価値反復を使ったアルゴリズムである。
現在の値を目標値に少しずつ更新していく手法は、強化学習の文脈において問題を解くために重要になる。
そのようなアルゴリズムとしてSarsaとQ-Learningを取り上げる。

### Sarsa
時刻$t$において状態$s_t$であり$a_t$を行った結果、次の状態$s\_{t+1}$と報酬$r_t$を観測したとき、$s\_{t+1}$において行う予定の行動$a\_{t+1}$をもとに、以下の更新則でQ値を更新する。

\begin{equation}
Q(s_t, a_t)
\leftarrow
(1 - \alpha) Q(s_t, a_t) + \alpha (r\_{t+1} + \gamma Q(s\_{t+1}, a\_{t+1}))
\end{equation}

$\alpha(0 \le \alpha &lt; 1)$は学習率と呼ばれるパラメータで、アルゴリズムの設計者が調整する。
このアルゴリズムは現在のQ値と$r\_{t+1} + \gamma Q(s\_{t+1}, a\_{t+1})$との内分によって、Q値を更新していく手法である。
先に述べたように、状態遷移確率を用いずに、行動によって発生する状態遷移の比率が、無限回試行していくごとに状態遷移確率に近づいていく性質を用いてQ値を求めている。

### Q-Learning
Sarsaと同様の設定で、以下の更新則でQ値を更新するのがQ-Learningである。

\begin{equation}
Q(s_t, a_t)
\leftarrow
(1 - \alpha) Q(s_t, a_t) + \alpha (r\_{t+1} + \gamma \max\_{a^{\prime} \in A} Q(s\_{t+1}, a^{\prime}))
\end{equation}

Sarsaと違って、次に何の行動を取ったかはQ値の更新には関わってこない。
$\max\_{a^{\prime} \in A} Q(s\_{t+1}, a^{\prime})$つまり、次の状態において取れる行動のうち、最大の価値を得られる行動(つまり最適方策$\pi^{\*}$)を採った時のQ値を使って更新する。
Sarsaが実際に取った方策に更新が依存するのに対して、Q-LearningではQ値は環境に対して一定の値に収束する。

このように実際に採用している方策と異なる方策を学習する方法は**方策オフ型**学習と呼ばれる。
一方で、Sarsaのような、学習した方策をそのときどき採用する方法を**方策オン**型学習と呼ぶ。


### 方策の決定方法
Q値が求まったら、最も高いQ値を持つ行動$a$を選択すれば、将来にわたって最大の報酬が得られることが期待できる。
つまり、行動$a^{\*}$は

$$
a^* = \arg\max(Q^{\*})
$$

となる方策を採用するということになる。
これはgreedy方策と呼ばれている。

さて、この方策では$Q^{\*}$がわかっていれば最適な方策となるが、強化学習では行動をしながら$Q$を更新していくので$Q^{\*}$は常に不明である。
探索をするために、ある程度ランダムな方策を取る必要がある。
そのように確率$\epsilon$でランダムな行動を取る方策を$\epsilon$-greedy方策という。 --&gt;
</description>
    </item>
    
    <item>
      <title>超人工生命ハッカソンに行ってきた</title>
      <link>http://blog.syundo.org/post/20160409Super-A-life/</link>
      <pubDate>Sat, 09 Apr 2016 10:17:51 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20160409Super-A-life/</guid>
      <description>

&lt;p&gt;4月9日(土)、&lt;a href=&#34;http://connpass.com/event/28982/&#34;&gt;超人工生命ハッカソン&lt;/a&gt;に行ってきた。&lt;/p&gt;

&lt;!-- 当日のスケジュールはこんな感じだった。

* 開会の言葉　山川宏
* チュートリアル 清水亮
* ソースコード詳細解説 中村政義
* ハッカソン開始 11:00
* 集合、ハッカソン終了 15:30
* 発表会 16:00
* 表彰 17:30
* 終了 18:00

## 超人工生命とは？
当日のチュートリアルでは主催者の一人であるUEIの清水亮氏から、人工知能 + 人工生命 = 超人工生命だ、という説明がされた。
人工生命といえば、セルオートマトンとかで数十年前にさんざん流行っていたが、最近あまり聞かなくなっていたと、清水氏は言う。近年の人工知能技術の人気の高まりに乗じて、人工生命のときと同じような盛り上がりにしようと、人工知能技術を使ったエージェントを「超人工生命」と呼ぶことにしたらしい。 --&gt;

&lt;h2 id=&#34;どんなのができるの:67d9104e26470c11657cf8933878c27f&#34;&gt;どんなのができるの?&lt;/h2&gt;

&lt;p&gt;こんなの。
&lt;img src=&#34;http://blog.syundo.org/images/2016/04/lis.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;緑の箱に触ると報酬GET&lt;/li&gt;
&lt;li&gt;黄色のボールに触ると報酬DOWN&lt;/li&gt;
&lt;li&gt;赤の箱に触ると死ぬ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;という状況の中で、左右どの方向に進むのか、ジャンプするのかを決めて勝手に動き回る。&lt;/p&gt;

&lt;p&gt;使っているのは、一人称視点のカメラの情報(とDepthセンサの深度情報)だけ！！！&lt;/p&gt;

&lt;p&gt;これはすごい！！！！！！&lt;/p&gt;

&lt;h3 id=&#34;セットアップ:67d9104e26470c11657cf8933878c27f&#34;&gt;セットアップ&lt;/h3&gt;

&lt;p&gt;ここらへんを見よう！&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wbap/lis&#34;&gt;https://github.com/wbap/lis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/nakamuu_m/items/2a2ba63cbd546ca2ea80&#34;&gt;http://qiita.com/nakamuu_m/items/2a2ba63cbd546ca2ea80&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mac環境でしかやってないけど、とくにハマることもなくできた。
シミュレーションが起動するまで数分待たないといけないので、根気よく待つ。&lt;/p&gt;

&lt;h3 id=&#34;簡単にできるカスタマイズ:67d9104e26470c11657cf8933878c27f&#34;&gt;簡単にできるカスタマイズ&lt;/h3&gt;

&lt;h4 id=&#34;アクションを増やす:67d9104e26470c11657cf8933878c27f&#34;&gt;アクションを増やす&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/wbap/lis/blob/master/python-agent/cnn_dqn_agent.py#L18&#34;&gt;この部分&lt;/a&gt; を&lt;/p&gt;
actions = [0,1,2,3]

&lt;p&gt;にしてみる。ジャンプもするようになるぞ！同じ要領で
&lt;a href=&#34;https://github.com/wbap/lis/blob/master/unity-sample-environment/Assets/Scripts/Action.cs#L14&#34;&gt;この部分&lt;/a&gt;
を変更すると、動作を追加できる。&lt;/p&gt;

&lt;h4 id=&#34;負の報酬も与えるようにする:67d9104e26470c11657cf8933878c27f&#34;&gt;負の報酬も与えるようにする&lt;/h4&gt;

&lt;p&gt;Hierarchey &amp;gt; SceneController &amp;gt; items にMinusRewardItem を追加する。
ResetItemは触ると死ぬようになるぞ！&lt;/p&gt;

&lt;h2 id=&#34;使われている技術:67d9104e26470c11657cf8933878c27f&#34;&gt;使われている技術&lt;/h2&gt;

&lt;p&gt;今回のハッカソンでは中村政義氏が開発した、「Lis」(リズ)というプログラムを使うことになっていた。Lisはpython側のサーバとUnity側のエージェントで構成されており、この２つはWebsocket通信によってやりとりして、うまく役割分担をしている。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;python側

&lt;ul&gt;
&lt;li&gt;Unity側から画像データを受け取る&lt;/li&gt;
&lt;li&gt;ImageNet + DQNを組み合わせたネットワークを使ってエージェントの行動を決定し、Unity側に送る。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Unity側

&lt;ul&gt;
&lt;li&gt;python側から行動の命令を受け取って、実際のエージェントの動作に反映する。&lt;/li&gt;
&lt;li&gt;エージェントの望ましい行動に対して与える報酬を決定する。&lt;/li&gt;
&lt;li&gt;エージェントに固定されたカメラから得られる画像と、報酬をpython側に送る&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;というような役割になっている。&lt;/p&gt;

&lt;h2 id=&#34;当日僕がやったこと:67d9104e26470c11657cf8933878c27f&#34;&gt;当日僕がやったこと&lt;/h2&gt;

&lt;h3 id=&#34;コードを読む:67d9104e26470c11657cf8933878c27f&#34;&gt;コードを読む&lt;/h3&gt;

&lt;p&gt;ハッカソンが始まると、まずpython側のコードを読んだ。DQNには以前から興味があったが、なかなか触る機会がなかったので、今回のハッカソンを機に親しみたいと思っていたからだ。以下はコードを読んだときのメモ。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ImageNetのプーリング層に入れる前の層(256 * 6 * 6 次元)をQ-Networkの入力にしている

&lt;ul&gt;
&lt;li&gt;なぜここなのかというと、物体がある位置のニューロンが活性化するようになっている層だかららしい。チュートリアルで、写真と、活性化している位置の画像を並べたのを見せてもらいながらそういった説明を受けた。いきなり生の画像を入力するのではなく、すでにチューニングされたネットワークを使うことで学習時間を短縮しようということなのか。なるほど。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ImageNetのほうは強化学習によって学習せず、DQNのほうのネットワーク(Q-Net)だけオンラインで学習していくようになっている。&lt;/li&gt;
&lt;li&gt;Q-Netは中間層が1層(255次元) のネットワーク

&lt;ul&gt;
&lt;li&gt;中間層1層なので全然Deepじゃない。複雑な行動を学習させていないから、これで表現力としては十分なのだろう。&lt;/li&gt;
&lt;li&gt;4step分の状態を使って学習できるようになっているが、デフォルトでは1stepだけの入力を使っている。&lt;/li&gt;
&lt;li&gt;action はデフォルトで3つ(左右旋回、前進)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ここまで理解したときには45分くらいが経っていた。
次にUnity側を読みに行った。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ランダムに環境を構築するようになっていた。&lt;/li&gt;
&lt;li&gt;アクションのコマンドをpython側から受けて、左右旋回、直進の動作に変換している&lt;/li&gt;
&lt;li&gt;カメラの画像をバイナリ列に変換して送っている。&lt;/li&gt;
&lt;li&gt;Cubeに触ると正のリワードや負のリワード、学習のリセット(死亡)を行うようになっている。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ここまでで1時間が経過した。この時点あたりから、実はUnityが触れないと結構つらいのではないかという予感が浮かんできた。&lt;/p&gt;

&lt;h3 id=&#34;パラメータいじり:67d9104e26470c11657cf8933878c27f&#34;&gt;パラメータいじり&lt;/h3&gt;

&lt;p&gt;コードを読み終わった後、Unity側の動作を変更してみた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;負のリワードを与える

&lt;ul&gt;
&lt;li&gt;負の報酬を与えるオブジェクトのモック(プレハブ?)はすでにUnityプロジェクト内に用意がしてあった。Unityの操作がわからず、30分くらい格闘してしまった。学習の初期段階だからなのか、ガンガンぶつかりに行っている。一晩置いてみたい。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ジャンプさせる

&lt;ul&gt;
&lt;li&gt;ジャンプするコードはすでにUnityプロジェクト側に用意してあったので、Q-Netから出力するactionの次元を3から4に変えてやればいいだけだった。&lt;/li&gt;
&lt;li&gt;段差のあるステージを用意するといいんだろうなと思って用意したが、短時間では面白い動作にはならなさそうだった。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;道路に沿って落ちないように移動する

&lt;ul&gt;
&lt;li&gt;ROBO-ONEの予選のような環境で、1本道を落ちないように移動することを学習させようとした。&lt;/li&gt;
&lt;li&gt;板のようなオブジェクトの上に、正の報酬を与える餌を置いてやって、落ちたら死ぬようにすればいいと考えた。だがしかし、結局Unityの書き方が理解できず、何もできなかった。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;発表会:67d9104e26470c11657cf8933878c27f&#34;&gt;発表会&lt;/h2&gt;

&lt;p&gt;いいものができた人17人を選抜して、一人3分で発表してもらうことになった。
名前などメモできていないので、ざっくりと紹介する。&lt;/p&gt;

&lt;h3 id=&#34;エージェントの行動と報酬をカスタマイズした人たち:67d9104e26470c11657cf8933878c27f&#34;&gt;エージェントの行動と報酬をカスタマイズした人たち&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;旋回と同時に前にも進むようにした&lt;/li&gt;
&lt;li&gt;時間経過とともにお腹が空くようにした&lt;/li&gt;
&lt;li&gt;伏し目にして下を見れるようにしてみた&lt;/li&gt;
&lt;li&gt;ダッシュ機能をつけた&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ニューラルネットワークを改良した人たち:67d9104e26470c11657cf8933878c27f&#34;&gt;ニューラルネットワークを改良した人たち&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;デフォルトではQ-Netへの入力として、AlexNetの比較的深い層を使っていたが、浅い層をQ-Netの入力にするようにして、Q-Netは層を増やしてCNNに変更した。

&lt;ul&gt;
&lt;li&gt;学習の進み具合がよくなったらしい。&lt;/li&gt;
&lt;li&gt;いろいろやり方はあるだろうけど、何がどう効いてくるのかはよくわからないなーと思った。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;推し松を追いかけるようにした。

&lt;ul&gt;
&lt;li&gt;おそ松さんの顔認識のためにトレーニングしたネットワークを使った(使おうとした)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://bohemia.hatenablog.com/entry/2015/11/22/174603&#34;&gt;おそ松さん判別器&lt;/a&gt;で話題になったぼへみあさん&lt;/li&gt;
&lt;li&gt;カラ松好きなエージェントを育てるなら、カラ松の顔画像が貼り付けられたボックスに接触すると報酬が高くなるようにする、とかそういうことをやっている。おもしろい。&lt;/li&gt;
&lt;li&gt;chainerのバージョンの違いで、そのまま使うことができず、当日トレーニングをし直したが、間に合わなかったとのこと。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;学習させる環境をカスタマイズした人たち:67d9104e26470c11657cf8933878c27f&#34;&gt;学習させる環境をカスタマイズした人たち&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ジャンプできるようにしてジャンプすれば取れる報酬とジャンプすると取れない報酬を用意して、ジャンプする行動としない行動を満遍なく学習できるようにした。&lt;/li&gt;
&lt;li&gt;自作ゲームと接続した

&lt;ul&gt;
&lt;li&gt;Unityなので簡単に組み合わせられるかというとそうでもなかったらしい。すごい。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;地面を球体にした

&lt;ul&gt;
&lt;li&gt;地平線の向こう側に餌があって見えないことがあるらしい。どういった行動が学習されるのか興味がある。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;エージェントを動いているターゲットを追いかける変質者にした

&lt;ul&gt;
&lt;li&gt;何を言っているかわからないかもしれないが、変質者だった。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ATARIのPONをプレイさせた

&lt;ul&gt;
&lt;li&gt;DQNといえば、ATARI。それを俯瞰視点を使わず、プレイヤー目線で3Dの環境で学習させたというのがおもしろい。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;パックマンみたいな環境にした

&lt;ul&gt;
&lt;li&gt;Unityがうまく使えず、完成しなかったらしい。気持ちよく分かる。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;スーパーマリオみたいな環境にした

&lt;ul&gt;
&lt;li&gt;今回優勝の人&lt;/li&gt;
&lt;li&gt;落ちたら死ぬ + 後ろから触れると死ぬ壁が追ってくる　環境を作ることでマリオっぽいものを学習させていた。&lt;/li&gt;
&lt;li&gt;短時間ながらそれっぽい動きを学習させることに成功していた。後ろを振り返りながら逃げる姿が超かわいい。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;マルチエージェントで追いかけっこするようにした

&lt;ul&gt;
&lt;li&gt;鬼はタッチすると正の報酬を得て、タッチされたほうは負の報酬になるようにした。&lt;/li&gt;
&lt;li&gt;会敵しやすいように。枠で囲って行動を制限していた。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;テクスチャを入れ替えた

&lt;ul&gt;
&lt;li&gt;skyboxに空のテクスチャを貼った&lt;/li&gt;
&lt;li&gt;cubeが光るようにした&lt;/li&gt;
&lt;li&gt;デフォルトの環境で学習させた後、この環境に投入してやるとどれくらいの時間で学習できるのか&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;実世界と接続する系:67d9104e26470c11657cf8933878c27f&#34;&gt;実世界と接続する系&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;照度センサの値をUnityに反映させる

&lt;ul&gt;
&lt;li&gt;実世界のセンサの値に応じてシミュレーション内に報酬を与えるようにした。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;iphoneで動かした

&lt;ul&gt;
&lt;li&gt;Unity側をiphoneで動くようにした&lt;/li&gt;
&lt;li&gt;カメラに映った画像に応じて報酬を与えるようにした。&lt;/li&gt;
&lt;li&gt;ついでにエージェントもUnityちゃんにした。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;短い時間だったが、かなり完成度が高いものを発表する人が多くて驚いた。
僕もUnityを使えるようになって引き続き触っていきたい。&lt;/p&gt;

&lt;p&gt;ではでは。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>セグメント液晶のことを全然知らなかった話</title>
      <link>http://blog.syundo.org/post/2016-02-28-LCDdisplay/</link>
      <pubDate>Sun, 28 Feb 2016 23:32:31 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/2016-02-28-LCDdisplay/</guid>
      <description>&lt;p&gt;この冬、私は急に太ってしまった。
寒さから食欲が増したことと、食事の時間が不規則な食生活に原因があると思われる。
腰回りの肉のせいで体を反らしにくくなったり、すぐ疲れるようになったりして、自覚するようになってきた。
今思えば、体重計が家に無いせいで、自分の体重についてそれほど気にしていなかったのも良くなかった。&lt;/p&gt;

&lt;p&gt;とりあえず体重計を買おうと思った。できればネットに記録を上げられるような良い奴が欲しい。
witnessというやつが今のところ一番良さそうだった。&lt;/p&gt;

&lt;p&gt;しかし、思ったより高かった。普通の体重計を改造してWifiモジュールを取り付けてやれば安価に作れるのではないか?と思った。&lt;/p&gt;

&lt;p&gt;早速、秋葉のヨドバシでタニタの3000円くらいの体重計(体脂肪計測機能つき)を買ってきた。
店頭にあった体重計を裏返して物色し、ネジで簡単に開けられそうなやつを選んだのだった。
果たして普通のドライバーでネジを回してやって、マイナスドライバーをちょっと差し込んでやるだけで、簡単に裏蓋を開けることができた。&lt;/p&gt;

&lt;p&gt;体重などの数値を取得するために、LCDに入力している部分を横から他のマイコンで読み取れないものかと算段していた。
体重計に付いている液晶は、7セグメント液晶と同じ原理なんだろうと、考えていた。
しかし蓋を開けてみてみれば、LCDのセグメント数と端子数は明らかに違った。
マトリクス状に回路が組んであって、2つの端子の組み合わせで、表示するセグメントを決めるのだろうと推察した。
ちょっと面倒だが、解析できなくもないだろうと思った。&lt;/p&gt;

&lt;p&gt;オシロスコープをマイコンからLCDにつながっている部分に当ててみると、なんと、500Hzほどのパルスが出ているではないか。
しかもPeek to Peek が 1v程度であった。
LCDをドライブするのにこんなことが行われいるなんて全く知らなかった私は非常に混乱した。&lt;/p&gt;

&lt;p&gt;結局、ネットを調べているうちに、こちらのブログ記事を発見した。
&lt;a href=&#34;http://jujurou.blog34.fc2.com/blog-entry-386.html&#34;&gt;LCD(TN &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; bias, &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; duty) を汎用I/Oポートで出力制御&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;バイアスとデューティーを巧みに設定して、各セグメントにおける平均電圧が0になるように入力が与えられるのか&amp;hellip;。&lt;/p&gt;

&lt;p&gt;こういった出力を用意することは、マイコンのLCDドライブ機能を使えば簡単にできるが、これを読み取るのは容易なことではない。
体重計のLCDの端子は18本だったので、18本のA/D変換を用意しなければならない。
さらに、時系列も考慮して、デューティー分の表示をタイミングよく記憶しておかなければならない。&lt;/p&gt;

&lt;p&gt;体重計のIoT化は、あきらめた。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>mbedのMPU6050のライブラリを更新した</title>
      <link>http://blog.syundo.org/post/2016-01-31-port-6050-library/</link>
      <pubDate>Sun, 31 Jan 2016 15:32:18 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/2016-01-31-port-6050-library/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/MPU6050&#34;&gt;こちらの&lt;/a&gt;Arduino用に書かれたコードからmbed用に移植したものを公開していたのだが、いかんせん何年も放ったらかしにしていたため、重い腰を上げて最新のコードに追従させた。特に何か大きく新しい機能が入ったとかは無いようで、リファクタリング程度の更新だった(とはいえ未だコードの品質は良いとは言えない&amp;hellip;)。
mbed用のライブラリは&lt;a href=&#34;https://developer.mbed.org/users/syundo0730/code/MPU6050/&#34;&gt;こちら&lt;/a&gt;。使用方法は&lt;a href=&#34;https://developer.mbed.org/users/syundo0730/code/MPU6050_Example/&#34;&gt;こちら&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;mbedのライブラリに加えた変更点は&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Arduino側のライブラリで変更があった箇所(接続テスト用の関数のようだが、なぜ追加したのかよくわからない)の修正&lt;/li&gt;
&lt;li&gt;I2Cに流すデータのフォーマットなどを決めている便利クラスI2Cdevのメンバ関数をArduinoに倣って全部staticにした。pinName をマクロで指定しなきゃいけないとか、複数のMPU6050を複数のi2cポートで使うのが難しくなるとかあるので、本当はやりたくなかったけど、Arduinoにコードを近づければ、ほぼコピペするだけで移植を終わらせることができるから、そうした。&lt;/li&gt;
&lt;li&gt;ArduinoのSerial.printをエミュレートするクラスを作った。これでArduinoのprintデバッグ部分をちょっと書き換えるだけで移植できるようになった。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Arduinoからmbedに移植するときに書き換えないといけないのは、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MPU6050.h : 48-52行のコメントアウトされているのを外す。なんでこれコメントアウトされているんだ？コンパイル通らない。&lt;/li&gt;
&lt;li&gt;MPU6050.cpp : 何もなし&lt;/li&gt;
&lt;li&gt;MPU6050_6Axis_MotionApps20.h : 103-108行 のデバッグ出力の部分をArduinoSerialに書き換える&lt;/li&gt;
&lt;li&gt;helper_3dmath.h : sqrtを使っているところを&lt;code&gt;sqrt((float)(x*x + y*y + z*z));&lt;/code&gt;に書き換える&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今後変更があっても簡単に追従できるようにできたのは良かった。
MPU9150も動かしたいというようなリクエストも今まで何回か来ていたので、MPU9150の方もちゃっちゃとやってしまおうかなと思っている。&lt;/p&gt;

&lt;p&gt;おわり。&lt;/p&gt;

&lt;h2 id=&#34;追記:eb2554d4c5dcb8776eef64d76a80df35&#34;&gt;追記&lt;/h2&gt;

&lt;p&gt;MPU9150のライブラリも移植した。MPU6050のために周辺を整備していたおかげで一瞬で移植できた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ライブラリ: &lt;a href=&#34;https://developer.mbed.org/users/syundo0730/code/MPU9150/&#34;&gt;https://developer.mbed.org/users/syundo0730/code/MPU9150/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用例: &lt;a href=&#34;https://developer.mbed.org/users/syundo0730/code/MPU9150_Example/&#34;&gt;https://developer.mbed.org/users/syundo0730/code/MPU9150_Example/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>良い物をメモしておく</title>
      <link>http://blog.syundo.org/post/2016-01-22-Goods-Memo/</link>
      <pubDate>Fri, 22 Jan 2016 23:57:58 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/2016-01-22-Goods-Memo/</guid>
      <description>

&lt;p&gt;リンクしておきたいメモ&lt;/p&gt;

&lt;h2 id=&#34;加工機:c0c3fb7bf9b2fd8954de5f5dd2c5a4fa&#34;&gt;加工機&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://gigazine.net/news/20160121-smart-laser-co2/&#34;&gt;低価格なレーザー加工機&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/toodooda/status/690979282038853633&#34;&gt;光造形式3Dプリンタ G Printer&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;電子部品:c0c3fb7bf9b2fd8954de5f5dd2c5a4fa&#34;&gt;電子部品&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://akizukidenshi.com/catalog/g/gM-10140/&#34;&gt;BLE無線モジュール&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://akizukidenshi.com/catalog/g/gM-08461/&#34;&gt;超小型USBシリアル変換モジュール&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HugoでGitHub markdown形式でコードブロックを記述する</title>
      <link>http://blog.syundo.org/post/2016-01-01-hugoFormula/</link>
      <pubDate>Fri, 01 Jan 2016 23:41:35 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/2016-01-01-hugoFormula/</guid>
      <description>&lt;p&gt;ふとブログにコードを載せようとしてmarkdown形式で書いてみたら反映されなかった。
Hugoに設定が要るらしい。&lt;/p&gt;

&lt;p&gt;Hugoをbrewからインストールして使っていたが、versionが古く0.14がインストールされていた。
最新のHugoをビルドして使う必要がある。&lt;/p&gt;

&lt;p&gt;まずgo を入れていなかったのでインストールする。&lt;/p&gt;
$ brew install go

&lt;p&gt;mercurial も入れていなかったのでインストール。&lt;/p&gt;
$ brew install mercurial

&lt;p&gt;最新のHugoをビルドして使う。&lt;/p&gt;
$ export GOPATH=$HOME/go
$ go get -v github.com/spf13/hugo

&lt;p&gt;$HOME/go/bin にHugoのバイナリが生成されるので、ここをPATHに追加しておく&lt;/p&gt;
$ export PATH=$PATH:$HOME/go/bin

&lt;p&gt;Hugoがちゃんと入ったか動作確認する。&lt;/p&gt;
$ hugo version
Hugo Static Site Generator v0.16-DEV BuildDate: 2016-01-01T23:34:08+09:00

&lt;p&gt;次にコードのハイライトを解析するPygmentsのインストールをする。&lt;/p&gt;
$ pip install Pygments

&lt;p&gt;config.tomlにPygmentsを使うことを設定する。
Pygmentsのスタイルのプレビューは公式サイトで見ることができる。&lt;a href=&#34;http://pygments.org/&#34;&gt;Pygments&lt;/a&gt;&lt;/p&gt;
pygmentscodefences = true
pygmentsstyle = &#34;paraiso-light&#34;

&lt;p&gt;ただし、Qiitaのmarkdown記法で&lt;code&gt;bash:filename&lt;/code&gt;とか書いてもファイル名は表示されない&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>意識はいつ生まれるのか</title>
      <link>http://blog.syundo.org/post/20151220NullaDiPiuGrande/</link>
      <pubDate>Sun, 20 Dec 2015 04:15:34 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/20151220NullaDiPiuGrande/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.amazon.co.jp/%E6%84%8F%E8%AD%98%E3%81%AF%E3%81%84%E3%81%A4%E7%94%9F%E3%81%BE%E3%82%8C%E3%82%8B%E3%81%AE%E3%81%8B%E2%80%95%E2%80%95%E8%84%B3%E3%81%AE%E8%AC%8E%E3%81%AB%E6%8C%91%E3%82%80%E7%B5%B1%E5%90%88%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96-%E3%82%B8%E3%83%A5%E3%83%AA%E3%82%AA%E3%83%BB%E3%83%88%E3%83%8E%E3%83%BC%E3%83%8B/dp/4750514500&#34;&gt;意識はいつ生まれるのか――脳の謎に挑む統合情報理論
ジュリオ・トノーニ (著), マルチェッロ・マッスィミーニ (著), 花本 知子 (翻訳)&lt;/a&gt;
を読んだ。&lt;/p&gt;

&lt;p&gt;この本で述べられていることはそのタイトル通りで、
脳を観測することで意識があるのか無いのかがわかるということ、
つまり意識がいつ生まれていつ消えているのかということだ。
結論から言うと、大脳の一部である視床-皮質系の情報処理の複雑度の高/低によって
意識のON/OFFが切り替わるらしい。
この視床-皮質系が何をするところとされているのかいまひとつよくわからなかったが、
視神経に直接関係するというわけでもないらしく、そこを計測してみたらそうなったということらしい。
切り替わる原因となるのは脳内に分泌される正イオン(カリウムイオン)による電位の変化で、
ニューロン間の信号の伝達が鈍くなることだと述べられている。
例えばノンレム睡眠のときはこれが分泌されるし、
麻酔の一種は脳内でカリウムを分泌する器官の働きを高めたりするらしいので、
意識の変化と関係があることは間違いない。&lt;/p&gt;

&lt;p&gt;筆者らは視床-皮質系の一部に電気的な刺激を加えて、
脳波計によって視床-皮質系のその他の部分が活性化するタイミングを計測した。
情報処理の複雑度をどう計算するのかは詳細が書いていなかったが、
「統合」と「情報」の合計値で評価するらしい。
「統合」とはあるニューロンに刺激が加えられたときにその他のニューロンが活性化するかということ。
「情報」とはあるニューロンに加えた刺激とその他のニューロンの活性の時系列的な差を示すものだ。
ノンレム睡眠時の脳とか脳死状態の患者の脳を計測して数値を計算すると、意識があるときの脳とはっきりとした違いが
出るというから面白い。&lt;/p&gt;

&lt;p&gt;ただ、この本で述べられているのは意識があるならば視床-皮質系の情報処理の複雑度が上がる、
ということだけで、いかにして意識を生み出すのかということは依然謎に包まれている。
視床-皮質系がぐちゃぐちゃと活性化しているときに行われている情報処理とは一体何なのか、
これがわかってくると意識を持った人工知能が実現できるかもしれない。&lt;/p&gt;

&lt;p&gt;おわり。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>マスタリングTPC/IP勉強</title>
      <link>http://blog.syundo.org/post/2015-11-23masteringTCPIP/</link>
      <pubDate>Tue, 24 Nov 2015 00:18:47 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/2015-11-23masteringTCPIP/</guid>
      <description>

&lt;h1 id=&#34;1章-ネットワーク基礎知識:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1章 ネットワーク基礎知識&lt;/h1&gt;

&lt;h2 id=&#34;1-3-プロトコルとは:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.3 プロトコルとは&lt;/h2&gt;

&lt;p&gt;TCP/IP &amp;hellip;IETFによって標準化されたもの&lt;/p&gt;

&lt;h3 id=&#34;1-5-3-osi参照モデル:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.5.3 OSI参照モデル&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;アプリケーション層

&lt;ul&gt;
&lt;li&gt;通信に関係するアプリケーション&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;プレゼンテーション層

&lt;ul&gt;
&lt;li&gt;データ形式に関するところ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;セッション層

&lt;ul&gt;
&lt;li&gt;データ転送に関するところ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;トランスポート層

&lt;ul&gt;
&lt;li&gt;宛先のアプリケーションにデータを確実に届ける役目&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ネットワーク層

&lt;ul&gt;
&lt;li&gt;宛先までデータを届ける役割&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データリンク層

&lt;ul&gt;
&lt;li&gt;物理層で直接接続されたノード間の通信を可能にする&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;物理層

&lt;ul&gt;
&lt;li&gt;ビット列を電圧の高低や光の点滅に変換したりそういうこと&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;1-7-通信方式の種類:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.7 通信方式の種類&lt;/h2&gt;

&lt;h3 id=&#34;1-7-1:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.7.1&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;コネクション型

&lt;ul&gt;
&lt;li&gt;データを送る前に送信ホストと受信ホストの間で回線の接続をする&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;コネクションレス型

&lt;ul&gt;
&lt;li&gt;通信相手を確認しない&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;1-8-ネットワークの構成要素:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8 ネットワークの構成要素&lt;/h2&gt;

&lt;h3 id=&#34;1-8-1-通信媒体とデータリンク:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8.1 通信媒体とデータリンク&lt;/h3&gt;

&lt;p&gt;ツイストケーブルとか&lt;/p&gt;

&lt;h3 id=&#34;1-8-2-ネットワーク-インターフェース:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8.2 ネットワーク・インターフェース&lt;/h3&gt;

&lt;p&gt;LANポートの部分&lt;/p&gt;

&lt;h3 id=&#34;1-8-3-リピーター:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8.3 リピーター&lt;/h3&gt;

&lt;p&gt;ネットワークを延長するもの。減衰した信号を増幅する&lt;/p&gt;

&lt;h3 id=&#34;1-8-4-ブリッジ-レイヤ2スイッチ:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8.4 ブリッジ/レイヤ2スイッチ&lt;/h3&gt;

&lt;p&gt;データリンク層&lt;/p&gt;

&lt;h3 id=&#34;1-8-5-ルーター-レイヤ3スイッチ:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8.5 ルーター/レイヤ3スイッチ&lt;/h3&gt;

&lt;p&gt;ネットワーク層&lt;/p&gt;

&lt;h3 id=&#34;1-8-6-ゲートウェイ:2834a11197f94bc4e7ce706eb948f310&#34;&gt;1.8.6 ゲートウェイ&lt;/h3&gt;

&lt;p&gt;トランスポート層からアプリケーション層までの階層で、データを中継する装置&lt;/p&gt;

&lt;h1 id=&#34;2章-tcp-ip-基礎知識:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2章 TCP/IP 基礎知識&lt;/h1&gt;

&lt;h2 id=&#34;2-1-tcp-ip登場の背景とその歴史:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2.1 TCP/IP登場の背景とその歴史&lt;/h2&gt;

&lt;h2 id=&#34;2-2-tcp-ipの標準:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2.2 TCP/IPの標準&lt;/h2&gt;

&lt;p&gt;プロトコルに対応してSTD(Standard) という番号が変わらない番号付けが付与される。
STDに対応するRFCの番号は規格の更新によって上下する。&lt;/p&gt;

&lt;h2 id=&#34;2-3-インターネットの基礎知識:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2.3 インターネットの基礎知識&lt;/h2&gt;

&lt;h3 id=&#34;2-3-2-tcp-ipとインターネット:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2.3.2 TCP/IPとインターネット&lt;/h3&gt;

&lt;p&gt;TCP/IP: インターネットを運用するために開発されたプロトコル&lt;/p&gt;

&lt;h3 id=&#34;2-3-3-インターネットの構造:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2.3.3 インターネットの構造&lt;/h3&gt;

&lt;p&gt;組織内ネットワーク =&amp;gt; NOC (Network Operation Center) =&amp;gt; IX (Internet Exchange)
IXを介して複数のNOCもとい組織が相互接続された巨大なネットワーク&lt;/p&gt;

&lt;h2 id=&#34;2-4-tcp-ip-プロトコルの階層モデル:2834a11197f94bc4e7ce706eb948f310&#34;&gt;2.4 TCP/IP プロトコルの階層モデル&lt;/h2&gt;

&lt;p&gt;トランスポート層(TCP): ヘッダ+(data)&lt;/p&gt;

&lt;h2 id=&#34;toc_19:2834a11197f94bc4e7ce706eb948f310&#34;&gt;|&lt;/h2&gt;
                       |

&lt;p&gt;ネットワーク層(IP): ヘッダ+(data)&lt;/p&gt;

&lt;h2 id=&#34;toc_20:2834a11197f94bc4e7ce706eb948f310&#34;&gt;|&lt;/h2&gt;
                                |

&lt;p&gt;トランスポート層(Ethernetなど): ヘッダ+(data)&lt;/p&gt;

&lt;p&gt;より下の階層のdata部に上の階層のheader+dataが格納される&lt;/p&gt;

&lt;h1 id=&#34;3章-データリンク:2834a11197f94bc4e7ce706eb948f310&#34;&gt;3章 データリンク&lt;/h1&gt;

&lt;p&gt;Ethernetとかの話。詳細は追わなくていいと思う&lt;/p&gt;

&lt;h1 id=&#34;4章-ipプロトコル:2834a11197f94bc4e7ce706eb948f310&#34;&gt;4章 IPプロトコル&lt;/h1&gt;

&lt;h3 id=&#34;4-2-1-ipアドレス:2834a11197f94bc4e7ce706eb948f310&#34;&gt;4.2.1 IPアドレス&lt;/h3&gt;

&lt;p&gt;インターネットに接続されるホストにはIPアドレスが付けられる。
IPアドレスをもとにしてIPパケットが配送される。&lt;/p&gt;

&lt;h3 id=&#34;4-2-2-ルーティング:2834a11197f94bc4e7ce706eb948f310&#34;&gt;4.2.2 ルーティング&lt;/h3&gt;

&lt;h4 id=&#34;パケット転送:2834a11197f94bc4e7ce706eb948f310&#34;&gt;パケット転送&lt;/h4&gt;

&lt;p&gt;ルータに問い合わせて次の転送先を決める。アドホック&lt;/p&gt;

&lt;h4 id=&#34;ルーティングテーブル:2834a11197f94bc4e7ce706eb948f310&#34;&gt;ルーティングテーブル&lt;/h4&gt;

&lt;p&gt;IPパケットを次にどのルーターに送ればいいのか記されている&lt;/p&gt;

&lt;h2 id=&#34;4-3-ipアドレスの基礎知識:2834a11197f94bc4e7ce706eb948f310&#34;&gt;4.3 IPアドレスの基礎知識&lt;/h2&gt;

&lt;h3 id=&#34;4-3-1-ipアドレスとは:2834a11197f94bc4e7ce706eb948f310&#34;&gt;4.3.1 IPアドレスとは&lt;/h3&gt;

&lt;p&gt;IPv4 &amp;hellip; 32bitの整数値&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>記号創発システムシンポジウムに行ってきた</title>
      <link>http://blog.syundo.org/post/souhatu20151123/</link>
      <pubDate>Mon, 23 Nov 2015 10:17:28 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/souhatu20151123/</guid>
      <description>

&lt;p&gt;ドワンゴで開催された&lt;a href=&#34;http://www.emergent-symbol.systems/news/1st_sympo_with_crest&#34;&gt;記号創発システムシンポジウム&lt;/a&gt;
に行ってきた。
ニコ生はこちら&lt;a href=&#34;http://live.nicovideo.jp/watch/lv242286012&#34;&gt;新鋭「人工知能×ロボティクス」プロジェクトによるシンポジウム&lt;/a&gt;
以下はメモ。でも途中でPCのバッテリーが切れてしまったので、午前中の分しか無い。&lt;/p&gt;

&lt;h1 id=&#34;記号創発システム論調査研究会設置にあたって:ced8b9337de5dd273e75e4e035be1676&#34;&gt;記号創発システム論調査研究会設置にあたって&lt;/h1&gt;

&lt;p&gt;谷口忠大先生（立命館大学）&lt;/p&gt;

&lt;p&gt;谷口先生による記号創発システムの重要性についての説明&lt;/p&gt;

&lt;h2 id=&#34;記号接地問題:ced8b9337de5dd273e75e4e035be1676&#34;&gt;記号接地問題&lt;/h2&gt;

&lt;h3 id=&#34;そもそも記号とは:ced8b9337de5dd273e75e4e035be1676&#34;&gt;そもそも記号とは&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;記号の恣意性&lt;/li&gt;
&lt;li&gt;真なる記号系が存在しているとか呈している&lt;/li&gt;
&lt;li&gt;与えられた記号系からそれに設置しようとするからだめ&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;何が問題だったか:ced8b9337de5dd273e75e4e035be1676&#34;&gt;何が問題だったか&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;記号とは何か

&lt;ul&gt;
&lt;li&gt;セミオーシス[Peirce]&lt;/li&gt;
&lt;li&gt;言語における記号も、サイン、解釈項、対象、これらがダイナミックに変わる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;じゃあどこから始めるべきか:ced8b9337de5dd273e75e4e035be1676&#34;&gt;じゃあどこから始めるべきか&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;主体のセンサー、身体

&lt;ul&gt;
&lt;li&gt;環世界 エルンスト＊マッハ　「感覚の分析」&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;記号創発システム論:ced8b9337de5dd273e75e4e035be1676&#34;&gt;記号創発システム論&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;記号が生まれる

&lt;ul&gt;
&lt;li&gt;記号を使って社会と関わる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;crest-記号創発ロボティクスによる人間機械コラボレーション基盤創成:ced8b9337de5dd273e75e4e035be1676&#34;&gt;CREST「記号創発ロボティクスによる人間機械コラボレーション基盤創成」&lt;/h1&gt;

&lt;p&gt;長井隆行先生（電気通信大学）&lt;/p&gt;

&lt;p&gt;プロジェクト概要&lt;/p&gt;

&lt;h2 id=&#34;全体像:ced8b9337de5dd273e75e4e035be1676&#34;&gt;全体像&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;概念、信念、応用&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;目指す世界:ced8b9337de5dd273e75e4e035be1676&#34;&gt;目指す世界&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;人とロボットの調和的, etc&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;２つのアプローチ:ced8b9337de5dd273e75e4e035be1676&#34;&gt;２つのアプローチ&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;DN, ベイズ&lt;/li&gt;
&lt;li&gt;従来のアプローチ

&lt;ul&gt;
&lt;li&gt;入力に対するラベルを人手でやる -&amp;gt; 認識&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;新たなアプローチ

&lt;ul&gt;
&lt;li&gt;教師なし学習&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;研究内容紹介1-概念班1:ced8b9337de5dd273e75e4e035be1676&#34;&gt;[研究内容紹介1（概念班1）]&lt;/h1&gt;

&lt;p&gt;尾形哲也先生（早稲田大学）&lt;/p&gt;

&lt;h2 id=&#34;背景:ced8b9337de5dd273e75e4e035be1676&#34;&gt;背景&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;DNNをロボットに応用する、ロボティクスへの応用はこれから

&lt;ul&gt;
&lt;li&gt;パターンを認識されても困る&lt;/li&gt;
&lt;li&gt;感覚と運動によってどう変わるかという概念獲得をしてほしい&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;マルチモーダル学習

&lt;ul&gt;
&lt;li&gt;画像、音声、全部を生データとしてDNNに入力する&lt;/li&gt;
&lt;li&gt;時間方向に引き伸ばして2000次元&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;記号と外界の結合

&lt;ul&gt;
&lt;li&gt;多義性を持った入力&lt;/li&gt;
&lt;li&gt;ちゃんと汎化してくれる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;まとめ:ced8b9337de5dd273e75e4e035be1676&#34;&gt;まとめ&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;パターンの有限数の符号への対応付け&lt;/li&gt;
&lt;li&gt;概念: パターンを生成した機構を推定し、予測、生成すること&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;研究内容紹介1-概念班2:ced8b9337de5dd273e75e4e035be1676&#34;&gt;[研究内容紹介1（概念班2）]&lt;/h1&gt;

&lt;p&gt;谷口忠大先生(立命館大学)&lt;/p&gt;

&lt;h2 id=&#34;記号接地:ced8b9337de5dd273e75e4e035be1676&#34;&gt;記号接地&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;家族しかわからないキーワード：お父さんの部屋&lt;/li&gt;
&lt;li&gt;ロボットのセンサ、モータ系と環境との相互作用の結果得られる高次元特徴量の性質を事前にしることは難しい&lt;/li&gt;
&lt;li&gt;何がグローバルな「常識」で何がローカルな「知識か」判別困難&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;mldaとnpylmを用いた物体概念と言語モデルの相互学習:ced8b9337de5dd273e75e4e035be1676&#34;&gt;MLDAとNPYLMを用いた物体概念と言語モデルの相互学習&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;語彙学習による物体カテゴリ形成、音声認識の高精度化

&lt;ul&gt;
&lt;li&gt;文節区切りと同時に物体概念を学習&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;場所概念と言語モデルの相互推定によるロボットの場所に関する語彙獲得

&lt;ul&gt;
&lt;li&gt;位置情報と語彙情報を統合することで精度向上&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;階層ディリクレ過程隠れ言語モデルに基づくノンパラ面トリックベイズ二重文節解析

&lt;ul&gt;
&lt;li&gt;言語モデルを知っているので文節解析できる&lt;/li&gt;
&lt;li&gt;音素からだけではなかなか精度が上がらないことが知られている&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;実環境での教師データ収拾の非効率性:ced8b9337de5dd273e75e4e035be1676&#34;&gt;実環境での教師データ収拾の非効率性&lt;/h2&gt;

&lt;p&gt;じゃあクラウドだ&lt;/p&gt;

&lt;h3 id=&#34;クラウドロボティクス:ced8b9337de5dd273e75e4e035be1676&#34;&gt;クラウドロボティクス&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;階層ベイズと組み合わせやすいのでは？&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;招待講演-記号創発ロボティクスに関する期待:ced8b9337de5dd273e75e4e035be1676&#34;&gt;招待講演「記号創発ロボティクスに関する期待」&lt;/h1&gt;

&lt;p&gt;安西祐一郎 先生 (独立行政法人日本学術振興会理事長)&lt;/p&gt;

&lt;h2 id=&#34;どうやって人間が自動機械の制御を学習するのか:ced8b9337de5dd273e75e4e035be1676&#34;&gt;どうやって人間が自動機械の制御を学習するのか&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;経緯

&lt;ul&gt;
&lt;li&gt;1984 年後半&lt;/li&gt;
&lt;li&gt;BP を実用する問題に関する書籍が甘利先生から出てブームになった&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベンチマークとして簡潔な問題を設定したほうがいいのではないか

&lt;ul&gt;
&lt;li&gt;問題と現実が乖離しすぎるんでないか&lt;/li&gt;
&lt;li&gt;蓄積をして良い成果を出してほしい&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;「意味」、「理解」の意味をしっかり把握したほうがいいのでは&lt;/li&gt;
&lt;li&gt;応用

&lt;ul&gt;
&lt;li&gt;実現可能であるのはプラクティカルに実現可能なのか&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;言語 とは

&lt;ul&gt;
&lt;li&gt;何のことか、全部を言っているなら大変なこと&lt;/li&gt;
&lt;li&gt;-&amp;gt; (言語，やりたいのは全部ですね．全部やらないとどうにもならん．)(谷口先生Twitterより)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;問題を明確にしようという話&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;船をゴールに向かって制御する問題:ced8b9337de5dd273e75e4e035be1676&#34;&gt;船をゴールに向かって制御する問題&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;すぐに操舵に反応しない。何分もかかって操舵する。習熟が必要。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;まとめ-1:ced8b9337de5dd273e75e4e035be1676&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;がんばってね、というお話&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>第8回ROS勉強会に参加してきた</title>
      <link>http://blog.syundo.org/post/ROS%E5%8B%89%E5%BC%B7%E4%BC%9A/</link>
      <pubDate>Sun, 04 Oct 2015 13:06:49 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/ROS%E5%8B%89%E5%BC%B7%E4%BC%9A/</guid>
      <description>

&lt;p&gt;2015/10/04に開催された、第８回ROS勉強会に参加してきた。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ros-users.doorkeeper.jp/events/30434&#34;&gt;https://ros-users.doorkeeper.jp/events/30434&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;dronecodeの概要とrosの対応について-himamura-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;Dronecodeの概要とROSの対応について @himamura さん&lt;/h2&gt;

&lt;p&gt;Droncodeとはオープンソースのフライトコントローラ(のソフト)のことらしい。
これのROS対応を進めているらしい。
Linuxを積んだボードなのでその中でROSが動く。&lt;/p&gt;

&lt;p&gt;遠隔操作をROSのメッセージでもって行うのはやっていないし、難しそうという話が会場から出た。&lt;/p&gt;

&lt;h2 id=&#34;kotlinでrosノードを動かしてみた-iwata-n-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;kotlinでROSノードを動かしてみた @iwata_n さん&lt;/h2&gt;

&lt;p&gt;slackのメッセージを受けてROSに送るプロキシみたいなものをkotlinで書いたということ。&lt;/p&gt;

&lt;h3 id=&#34;何をするのか:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;何をするのか&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;ROSJavaのインストール&lt;/li&gt;
&lt;li&gt;kotlinを書く&lt;/li&gt;
&lt;li&gt;intelliJを使ってkotlinからJavaに変換する&lt;/li&gt;
&lt;li&gt;ROSの上でkotlinが動く&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;kotlinはJavaを書くよりマシだというだけでなく、
pythonより幾分か素敵な文法と機能を持っているという点で魅力的だと思う。&lt;/p&gt;

&lt;p&gt;ただ、文法がC的でないという理由で馴染めない人も多かろうからROSでは流行らない。&lt;/p&gt;

&lt;h2 id=&#34;sphinxcontrib-ros-otamasan-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;sphinxcontrib-ros @otamasan さん&lt;/h2&gt;

&lt;p&gt;ROSのドキュメント生成をソースのクラス定義などを使って一部自動化したという話。
ROSのドキュメントの置き場、生成法にはいろいろ派閥があるそうで、新しい選択肢として名乗りを上げてほしい。&lt;/p&gt;

&lt;h2 id=&#34;自律型生活支援ロボットmini-ken-demu-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;自律型生活支援ロボットMini @ken_demu さん&lt;/h2&gt;

&lt;p&gt;発表者が17歳だった。
これを17歳が作れるのかというのがにわかには信じられない。
圧倒されて会場の空気が変わってた。
ものすごい実装力。
オープンソースのものをうまく組み合わせて高度な機能を実現していた。&lt;/p&gt;

&lt;h2 id=&#34;テーブルトップ型対話ロボットsotaの紹介-masato-ka-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;テーブルトップ型対話ロボットSotaの紹介 @masato_ka さん&lt;/h2&gt;

&lt;p&gt;NTTデータのロボット事業の紹介。
SotaとNTT研究所の研究成果を組み合わせた例。
高齢者見守りなどに使えるらしい。
例えば夜間に寝床を抜けて徘徊しそうなとき、声をかけてそれを止めたりする。&lt;/p&gt;

&lt;p&gt;最後に未来館でのデモを実演してくれた。
Sotaはまだ一般には売られていないが、将来的に10万程度で買えるらしい。
かわいい。ほしい。&lt;/p&gt;

&lt;h2 id=&#34;euslispでロボットプログラミング-hyaguchijsk:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;euslispでロボットプログラミング @hyaguchijsk&lt;/h2&gt;

&lt;p&gt;jskではこれが使われているんだなぁ。
一部だろうけど。
機械系出身のひとにはいきなりLispはきつかろうと思う。&lt;/p&gt;

&lt;p&gt;でも東大生ならなんてことないのかもしれない。
会場では取り残されている人が続出していた様子だ。&lt;/p&gt;

&lt;h2 id=&#34;ros2-0時代に備えるためのc-11-14-otl-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;ROS2.0時代に備えるためのc++11, 14 @OTL さん&lt;/h2&gt;

&lt;p&gt;発表中に挙手でアンケートが取られたのだが、
C++11, 14 を普段から使っている人は0だった。&lt;/p&gt;

&lt;p&gt;私はここ半年触っていないなぁと思ったので手を挙げなかったが、
ここはC++勉強会で無かったのでC++14, 17とかいうものの存在を知っているだけで珍しいのだった。
カルチャーショックだ。&lt;/p&gt;

&lt;h2 id=&#34;rviz-rqtなどによるオンライン可視化-ログデータのオフライン可視化-仮-garaemon-coder-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;rviz、rqtなどによるオンライン可視化、ログデータのオフライン可視化(仮) @garaemon_coder さん&lt;/h2&gt;

&lt;p&gt;可視化はマルチスレッドのプログラムになるから難しいが、ROSのpub/sub型の通信方式によって可視化がしやすくなった&lt;/p&gt;

&lt;h3 id=&#34;どの方法を使って可視化をするか:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;どの方法を使って可視化をするか&lt;/h3&gt;

&lt;h4 id=&#34;タイムスタンプが重要かどうか:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;タイムスタンプが重要かどうか&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;重要 =&amp;gt; オフラインでの可視化

&lt;ul&gt;
&lt;li&gt;rqt_bag&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;重要でない =&amp;gt; オンラインでの可視化

&lt;ul&gt;
&lt;li&gt;rqt &amp;lt;- 基本的に使わない。rvizに2次元情報をオーバーレイすればいいから。&lt;/li&gt;
&lt;li&gt;rviz &amp;lt;- プラグインを使ってカスタマイズすればとても便利に&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JSKのリポジトリを見れば幸せになれる。&lt;/p&gt;

&lt;p&gt;気になったもの&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;image_view2

&lt;ul&gt;
&lt;li&gt;画像を表示するだけでなく、マウスでクリックした座標などがトピックとして取得できる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;footstepの可視化のやつ(名前忘れた)

&lt;ul&gt;
&lt;li&gt;初期位置とゴールを手動で指定してやれば、その間の歩行計画をした結果を可視化してくれる&lt;/li&gt;
&lt;li&gt;グリッド状にばらまいたノードも可視化されていて、Footstep planningに覚えがある人にはたまらない画面だった。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;osxではじめるrosプログラミング-youtalk-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;OSXではじめるROSプログラミング @youtalk さん&lt;/h2&gt;

&lt;p&gt;OSをアップデートしたらXcodeがアップデートされてしまってROS環境が壊れてしまったらしい。
Macだめじゃん。みたいな空気で終わってしまった。&lt;/p&gt;

&lt;h2 id=&#34;rosと機械学習でroombaを覚醒させたい-準備編-longjie0723-さん:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;ROSと機械学習でRoombaを覚醒させたい（準備編） @longjie0723 さん&lt;/h2&gt;

&lt;p&gt;RNNを教師あり学習で学習して赤外線センサとバンパセンサの情報を使って自動走行を実現していた。
PFNの例の車のデモを1台で、教科学習でなくて人手で行ったもの。&lt;/p&gt;

&lt;h2 id=&#34;おわりに:cdcb53ee6616dd01ebf332c0122baaf3&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;面白い発表ばかりだった。ヴァニラをROS対応して勉強会で発表できる日が来るのかなぁ。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ROBO-ONE 27th</title>
      <link>http://blog.syundo.org/post/robo-one-27th/</link>
      <pubDate>Sat, 03 Oct 2015 22:46:10 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/robo-one-27th/</guid>
      <description>&lt;p&gt;第27回ROBO-ONEの予選を見に行ってきた。
今回はヴァニラはエントリーしていなかったが、人形使いさんのチームメンバーという形で大会に顔を出させていただいた。&lt;/p&gt;

&lt;p&gt;予選は安定して4.5m踏破できるロボットが増えてきた印象だった。
自分で確かめてはいないが、話を聞くところによると、床に置かれた障害物はゴムシートとプラスチックの板？のようだということだった。
今までは幅の狭いゴムのシールが貼ってあっただけだったので、難しくなったと言える。
しかし、ちゃんと足を上げて歩行していれば今までと同じように進むことはできるようだった。&lt;/p&gt;

&lt;p&gt;Frostyさんの速さといったら、なんということだろう。&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/s81OgcZzpd4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;西村さんは予選の総括として、自律性の向上を推奨したいと仰っていた。
自動で避けろという問いかけで、避けなければ通れないような障害物が置かれるときが来るかもしれない(ないない)。&lt;/p&gt;

&lt;p&gt;今回のロボワンで(特にドール勢の)注目の的になっていたのが、真広さんのST-00だ。
予選の動画を見てもわかるようにものすごい完成度だ。
遠目で見ると本物のHRP-4Cが歩いているようにしか見えない。&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/nc5T3ACm6hM&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;予選が終わった後、念願のドールロボ大集合！写真撮影をした。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.syundo.org/images/2015/10/ドール集合写真.jpg&#34; alt=&#34;ドール集合写真&#34; title=&#34;ドール集合写真&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ROBO-ONEの歴史の中でこれだけのドールロボが集まったのはこれが初めてだろう。
そして最後かもしれない。
左から順に、びいむさん @beam8000 の「まい」ちゃん、
舞鈴堂さん @Blade_Oh の「音叉」ちゃん、「剣姫」ちゃん、
真広さんの「ST-00」ちゃん、
きゅんどう @ksyundo の「ヴァニラ」、
人形つかいさん @witch_kazumin の「茉莉花」ちゃん、
のむむさん @umumon の「AMATERAS」様、「ゴエン」さん。&lt;/p&gt;

&lt;p&gt;のむむさんのAMATERASさんなんかはドールロボの道を切り開いたとも言える名前通りの神的な存在なので、
一緒のフレームにうちのヴァニラが映ることができて、とっても嬉しかった。&lt;/p&gt;

&lt;p&gt;今年も人形つかいさん茉莉花ちゃんと写真を撮ることができた。
前回は僕がドール外皮を付けていかなかったため1年ぶりだ。
ドール集合を呼びかけていただいて、大変良かったです。
いつもお世話になってます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.syundo.org/images/2015/10/まりかちゃんと鎧.jpg&#34; alt=&#34;まりかちゃんと鎧&#34; title=&#34;まりかちゃんと鎧&#34; /&gt;&lt;/p&gt;

&lt;p&gt;うちの娘は顔の外皮がすぐズレちゃうので目が怖い。あと眼球が銀色のままだから(usbカメラ)怖い。塗らなければ。&lt;/p&gt;

&lt;p&gt;前においてあるのは人形つかいさんが購入された鎧ボトルカバー。
ヴァニラには着れないけど、これでバトル出場されたら応援したくなること間違いない。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.syundo.org/images/2015/10/アマテラスと茉莉花ちゃん.jpg&#34; alt=&#34;アマテラスと茉莉花ちゃん&#34; title=&#34;アマテラスと茉莉花ちゃん&#34; /&gt;&lt;/p&gt;

&lt;p&gt;このようにMSDサイズ勢は兜をちゃんと被れる。かわいい。&lt;/p&gt;

&lt;p&gt;今回、ヴァニラを運ぶために、MSDドール用のキャリングケースを導入した。
&lt;a href=&#34;http://www.dollfiedream.tokyo/jp/dollfiedream/case.html/&#34;&gt;ドール用キャリングケース&lt;/a&gt;
秋葉原のラジオ会館、ドールスポットに在庫があったので買った。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.syundo.org/images/2015/10/ドールバッグ.jpg&#34; alt=&#34;ドールバッグ&#34; title=&#34;ドールバッグ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;かなりホラーだけどこんな感じに膝を曲げればすっぽりと入る。
バトルロボットだったら肩幅がきつくて入らないだろうな。&lt;/p&gt;

&lt;p&gt;今回のロボワンはドールロボ製作者の方と集まれて楽しかった。
(ロボワン的にこれ盛り上がるのはどうなの？？って気はするけどｗｗ)
次の大会は出たい。
そしてそれがヴァニラの最後の出場になるといいと思っている。
1年とちょっとを見込んで新しいドールロボを作っていくつもりだ。
名前は「ソルト」。男みたいな名前だけどもちろん女性形にする。&lt;/p&gt;

&lt;p&gt;真広さんのST-00を見て、本当に衝撃を受けた。
同時にどうして自分にあれが作れないのかという悔しい気持ちにもなった。&lt;/p&gt;

&lt;p&gt;今は少なくともサーボモータは自分で作らなければ、自分が満足して開発できるものができないなという気持ちになっている。
作っても仕方がないという気持ちでは趣味のロボットの開発など続けられないのである。
技術的に更にチャレンジもしていきたいという考えもある。
サーボケースを自分で削りだすことはマストだ。
僕は機械設計にそれほど強くなく(センスが無いと言っても良い)、メカ的なことはなるべく避けてきた。
でもやらなきゃ仕方がないから、フルスクラッチすることを目指して作業を進めていくことに決めた。
１年かけて基礎技術の研究と設計をしていくつもり。&lt;/p&gt;

&lt;p&gt;次回のロボワンはヴァニラを使って歩行安定制御の検証をしたい。
同時に俺サーボのプロトタイプのプロトタイプ程度のものができていたらいいなと思う。&lt;/p&gt;

&lt;p&gt;もちろん仕事関係でやりたいこと、勉強しときたいこと死ぬほどあるからバランスよくやる。
redmineでも導入してタスク管理してみるといいかもしれない。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haskellで高階関数を組み合わせて部分文字列を作る例が全然わからなかったから考えた</title>
      <link>http://blog.syundo.org/post/1162/</link>
      <pubDate>Thu, 21 May 2015 22:27:54 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/1162/</guid>
      <description>&lt;p&gt;最近、Haskellの勉強のために「&lt;a href=&#34;http://www.amazon.co.jp/%E9%96%A2%E6%95%B0%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80-%E2%94%80%E2%94%80%E7%B0%A1%E6%BD%94%E3%81%A7%E3%80%81%E6%AD%A3%E3%81%97%E3%81%84%E3%82%B3%E3%83%BC%E3%83%89%E3%82%92%E6%9B%B8%E3%81%8F%E3%81%9F%E3%82%81%E3%81%AB-WEB-PRESS-plus/dp/4774169269&#34;&gt;関数プログラミング実践入門&lt;/a&gt;」を読んでいる。&lt;/p&gt;

&lt;p&gt;高階関数のところまでいったけど僕の頭がクソ雑魚だから例題が全然理解出来なかった…。&lt;/p&gt;

&lt;p&gt;Qiitaに記事を移しました。&lt;/p&gt;

&lt;p&gt;続きはこちら
&lt;a href=&#34;http://qiita.com/nekokoneko_mode/items/724b7e9bddac58e78e83&#34;&gt;http://qiita.com/nekokoneko_mode/items/724b7e9bddac58e78e83&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>C&#43;&#43;初心者会に参加してきた</title>
      <link>http://blog.syundo.org/post/1154/</link>
      <pubDate>Mon, 18 May 2015 01:26:10 +0900</pubDate>
      
      <guid>http://blog.syundo.org/post/1154/</guid>
      <description>&lt;p&gt;歌舞伎座.tech#8「C++初心者会」に参加してきた。
&lt;a href=&#34;http://kbkz.connpass.com/event/13905/&#34;&gt;http://kbkz.connpass.com/event/13905/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;初めて勉強会での発表ということもした（ただしLT)&lt;/p&gt;

&lt;p&gt;発表資料はこちら
&lt;a href=&#34;http://www.slideshare.net/syundo/c-48237620&#34;&gt;http://www.slideshare.net/syundo/c-48237620&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ニコ生はこちら
&lt;a href=&#34;http://live.nicovideo.jp/gate/lv220960718&#34;&gt;http://live.nicovideo.jp/gate/lv220960718&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;メモを取って即時公開してくださっているひとがいた。
&lt;a href=&#34;http://www.shigemk2.com/archive/category/C%2B%2B&#34;&gt;http://www.shigemk2.com/archive/category/C%2B%2B&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;私はLTでC++を使って開発できるマイコンボードの例としてmbedを紹介した。
だが、問題はmbedのコンパイラにあった。
C++界隈の人たちの興味はC++11，14、さらにはC++17にあるのだ。
対してmbedオンラインコンパイラはC++98だ。
C++98のことなんてもう忘却の彼方にあるし、取るに足りないことなのだ（実際僕もそう思う）。&lt;/p&gt;

&lt;p&gt;mbedの開発環境は不自由だ。
ネット環境がないとコンパイルできないとかザコだ。
なぜそんな環境に拘っているのか？
そのご指摘はまさにその通りだと思う。&lt;/p&gt;

&lt;p&gt;arm向けの自由なコンパイラがあるんだから、mbedはオンラインコンパイラに頼るエコシステムにすべきじゃなかったのだ。
オンラインコンパイラが無ければmbedは広まらなかったか？
僕はそうは思わない。Arduinoの開発環境はローカルの開発環境だが超流行っているじゃないか。
Coretex-M3の強い計算機能力を発揮して楽しくC++開発できることがmbedの強みだ。
Arduinoなんて目じゃない。&lt;/p&gt;

&lt;p&gt;それなのにコンパイラがC++98/03準拠というのはあんまりじゃないか。
mbedのコミュニティのことを考え、自分のしたい開発のことを考え、もやもやしている&amp;hellip;&lt;/p&gt;

&lt;p&gt;C++界隈の人たちというのは濃い、強い。
自分ももっと技術力を高めて本当のプログラマになりたいと思った。&lt;/p&gt;

&lt;p&gt;これまで聞いたこと無いキーワードや，聞いたことあったがよく知らないことをたくさん知れた。
調べて勉強してもっとC++をわかりたい。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>