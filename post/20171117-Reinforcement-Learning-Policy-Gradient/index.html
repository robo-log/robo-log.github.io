
<!DOCTYPE html>
<html lang="ja">
<head>

  
  <meta charset="UTF-8">
  <title>
    強化学習についてまとめる(3) 方策勾配 | ROBO LOG
  </title>


  
  <meta name="viewport" content="width=device-width,user-scalable=no,maximum-scale=1,initial-scale=1">

  
  <link rel="canonical" href="http://blog.syundo.org/post/20171117-reinforcement-learning-policy-gradient/"/>

  
  <link rel="stylesheet" href="/css/sanitize.css">
  <link rel="stylesheet" href="/css/responsive.css">
  <link rel="stylesheet" href="/css/highlight_monokai.css">
  <link rel="stylesheet" href="/css/theme.css">
  <link rel="stylesheet" href="/css/custom.css">

  
  <link href="http://blog.syundo.orgindex.xml" rel="alternate" type="application/rss+xml" title="ROBO LOG" />
  <link href="http://blog.syundo.orgindex.xml" rel="feed" type="application/rss+xml" title="ROBO LOG" />

  
	<script type="text/javascript">var switchTo5x=true;</script>
	<script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
	<script type="text/javascript">stLight.options({publisher: "02e11161-0b6b-46d7-9b85-d7ed58578dfd", doNotHash: true, doNotCopy: true, hashAddressBar: false});</script>


</head>



<body>
<div class="container">

  
  <header role="banner">
    <div class="row gutters">
      <div id="site-title" class="col span_6">
        <h1><a href="http://blog.syundo.org">ROBO LOG</a></h1>
        <h2>つくりたいものをつくる</h2>
      </div>
      <div id="social" class="col span_6">
        <ul>
          <li><a href="https://twitter.com/ksyundo" target="_blank">Twitter</a></li>
          
          <li><a href="https://github.com/syundo0730" target="_blank">GitHub</a></li>
          
        </ul>
      </div>
    </div>
  </header>


  
  <main id="single" role="main">
    <div class="article-header">
      <h1>強化学習についてまとめる(3) 方策勾配</h1>
      <div class="meta">
        Nov 17, 2017 &nbsp;
        
      </div>
    </div>
    <article>
      

<!-- 強化学習について古典的なものからDeepNNを使ったものまでまとめていきたい。
<!-- ![](/images/2016/04/lis.gif) -->

<!-- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)を土台にして他文献を参照しながら構成していく。
取り上げる順番はこだわらず、自分のわかりやすい方法を採る。
# ロードマップ  -->

<p><a href="../20171110-Reinforcement-Value-Policy-Iteration">前回、</a>
<a href="../20160410-Reinforcement-Learning-MDP-Belman-Equation">前々回</a>
では、価値関数を求め、それを基に行動を決定する手法について扱ってきた。
しかし、そもそもロボットの行動を決めるのは方策であるのだから、その方策を直接学習できないのだろうか？という疑問が湧く。</p>

<p>今回は、前回までとは全く違うアプローチとして、<strong>方策勾配法</strong> をまとめる。
方策勾配法は、方策をあるパラメタで表される関数とし、そのパラメタを学習することで、直接方策を学習していくアプローチである。</p>

<p>方策を直接扱うことで</p>

<ul>
<li>$V^{\pi}$や$Q^{\pi}$を求めるような複雑でメモリを消費する手法を使わなくて良い</li>
<li>連続空間での行動が扱いやすくなる</li>
</ul>

<p>などの利点がある。
一方で、</p>

<ul>
<li>方策オン型の学習になるため、行動のタイミングで方策を更新しなくてはならず、学習効率が落ちる</li>
<li>行動に対して学習が敏感に影響を受けてしまうため、学習ステップの決定が難しい</li>
</ul>

<p>などの欠点がある。</p>

<h1 id="方策のモデルと勾配">方策のモデルと勾配</h1>

<p>$\theta$でパラメタライズされた確率的な方策$\pi_{\theta}$を求める問題を考える。
$\tau$をステップ$0$から$H$までの状態-行動の系列(状態-行動空間でのパス)$\tau=(s_0, a_0, \dots, s_H, a_H)$としたとき、方策の評価関数として以下を考える。</p>

<p>\begin{equation}
\begin{aligned}
U(\theta) &amp;=&amp; E [\sum_{t=0}^H R(s_t, u_t) ; \pi_{\theta}] \\<br />
&amp;=&amp; \sum_{\tau} P(\tau ; \theta) R(\tau)
\end{aligned}
\end{equation}
ここで、$R(\tau) = \sum_{t=0}^H R(s_t, u_t)$としている。
また$P(\tau ; \theta)$はパスの生成モデルであり、定義より
\begin{equation}
P(\tau ; \theta) = \prod_{t=0}^H P(s_{t+1} | s_t, u_t) \pi_{\theta}(u_t | s_t)
\label{eq:p_tau_theta}
\end{equation}
である。</p>

<p>以上の設定において、方策の学習は最終的に
\begin{equation}
\max_{\theta} U(\theta) = \max_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau)
\end{equation}
を求める問題となる。
そこで、微小ステップごとに評価関数の$\theta$での微分方向
\begin{equation}
\nabla_{\theta} U(\theta) = \nabla_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau)
\end{equation}
に方策を更新することで、方策を最適化することを考える。
これは以下のように変形できる。</p>

<p>\begin{equation}
\begin{aligned}
\nabla_{\theta} U(\theta) &amp;=&amp; \nabla_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau) \\<br />
&amp;=&amp; \sum_{\tau} \nabla_{\theta} P(\tau ; \theta) R(\tau) \\<br />
&amp;=&amp; \sum_{\tau} \frac{P(\tau ; \theta)}{P(\tau ; \theta)} \nabla_{\theta} P(\tau ; \theta) R(\tau) \\<br />
&amp;=&amp; \sum_{\tau} P(\tau ; \theta) \frac{\nabla_{\theta} P(\tau ; \theta)}{P(\tau ; \theta)} R(\tau) \\<br />
&amp;=&amp; \sum_{\tau} P(\tau ; \theta) \nabla_{\theta} \log P(\tau ; \theta) R(\tau) \\<br />
\end{aligned}
\end{equation}</p>

<p>$P(\tau ; \theta)$で期待値を取る操作は、以下の平均を取る操作として近似する。
\begin{equation}
\nabla_{\theta} U(\theta)
\approx
\hat{g} = \frac{1}{m} \sum_{i=0}^m \nabla_{\theta} \log P(\tau^{(i)} ; \theta) R(\tau^{(i)})
\end{equation}</p>

<p>この式を見ると、更新則によって、</p>

<ul>
<li>報酬$R$が高いパスの存在確率を上げる</li>
<li>報酬$R$が低いパスの存在確率を下げる</li>
</ul>

<p>方向に方策が更新されることがわかる。
これをパスではなく方策と遷移確率について立式するとどうなるだろうか。
式\eqref{eq:p_tau_theta}より、</p>

<p>\begin{equation}
\begin{aligned}
\nabla_{\theta} \log P(\tau^{(i)} ; \theta)
&amp;=&amp; \nabla_{\theta} \log
\left[ \prod_{t=0}^H P(s_{t+1}^{(i)} | s_t^{(i)}, u_t^{(i)}) \pi_{\theta}(u_t^{(i)} | s_t^{(i)}) \right] \\<br />
&amp;=&amp; \nabla_{\theta} \left[ \sum_{t=0}^H \log P(s_{t+1}^{(i)} | s_t^{(i)}, u_t^{(i)}) \right] +
\nabla_{\theta} \left[ \sum_{t=0}^H \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) \right] \\<br />
&amp;=&amp; \nabla_{\theta} \sum_{t=0}^H \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) \\<br />
&amp;=&amp; \sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)})
\end{aligned}
\end{equation}</p>

<p>となる。状態遷移確率$P(s_{t+1}^{(i)} | s_t^{(i)}, u_t^{(i)})$は$\theta$をパラメタとして持たないので、$\theta$で微分するとこの項が消えるのである。</p>

<p>結局、勾配は
\begin{equation}
\begin{aligned}
\hat{g} = \frac{1}{m} \sum_{i=0}^m \nabla_{\theta} \log P(\tau^{(i)} ; \theta) R(\tau^{(i)}) \\<br />
where \hspace{15pt}
\nabla_{\theta} \log P(\tau^{(i)} ; \theta) =
\sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)})
\end{aligned}
\end{equation}</p>

<p>すなわち
\begin{equation}
\hat{g} = \frac{1}{m} \sum_{i=0}^m \sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) R(\tau^{(i)})
\label{eq:policy_gradient}
\end{equation}</p>

<p>で表され、方策の$\theta$での勾配のみで表現できることがわかる。</p>

<h2 id="baseline">Baseline</h2>

<p>式\eqref{eq:policy_gradient}にbaseline $b$という値を追加する。
\begin{equation}
\hat{g} = \frac{1}{m} \sum_{i=0}^m \sum_{t=0}^H \nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) (R(\tau^{(i)}) - b)
\label{eq:base_lined_policy_gradient}
\end{equation}</p>

<p>baselineは$\hat{g}$の値には影響を与えないが、調整前に比べて$\nabla_{\theta} \log \pi_{\theta} (u_t^{(i)} | s_t^{(i)}) (R(\tau^{(i)}) - b)$の分散を減少させる効果がある。</p>

<p>$R(\tau^{(i)}) - b$が小さいほど分散が小さくなるので、$b$の決定法としては、$R(\tau^{(i)})$との二乗距離を小さくするように調整すればよい。</p>


      
			<div id="share-this" class="col span_10">
				<span class='st_twitter_large' displayText='Tweet'></span>
				<span class='st_facebook_large' displayText='Facebook'></span>
				<span class='st_hatena_large' displayText='Hatena'></span>
				<span class='st_linkedin_large' displayText='LinkedIn'></span>
				<span class='st_email_large' displayText='Email'></span>
			</div>
    </article>
    
 <aside><div id="disqus_thread"></div></aside> 

<script type="text/javascript">
     
    var disqus_shortname = 'robolog';

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  </main>

  <nav class="pagination">
    
      <span class="previous">&larr; <a href="http://blog.syundo.org/post/20171110-reinforcement-value-policy-iteration/" rel="prev">強化学習についてまとめる(2) 反復による価値の推定</a></span>
    
    
      <span class="next"><a href="http://blog.syundo.org/post/20171202-reinforcement-learning-policy-gradient-algorithms/" rel="next">強化学習についてまとめる(4) 方策勾配に基づくアルゴリズム、Actor-Critic</a> &rarr;</span>
    
  </nav>


  
  <footer role="contentinfo">
    <div style="text-align:center;">
      <img src="/images/profile.jpeg" width="64" height="64"><br>
      Written by Shundo Kishi
    </div>
  </footer>


</div>

<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-21014159-4', 'auto');
	ga('send', 'pageview');
</script>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    displayAlign: "left",
    displayIndent: "2em",
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    TeX: { equationNumbers: { autoNumber: "AMS" }}
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
  });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
  });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>

